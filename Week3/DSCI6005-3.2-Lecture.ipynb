{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Using Neural Nets for Image Classification - Part 1\n",
    "\n",
    "##What you will learn today\n",
    "1.  Basics of backpropagation - mechanics of implementing gradient descent\n",
    "2.  Network output layer architecture for multi-class classification\n",
    "3.  Use of softmax and cross entropy functions\n",
    "4.  Optimizing network performance\n",
    "\n",
    "##Order of topics\n",
    "1.  Back Propagation\n",
    "2.  Description of MNIST data set\n",
    "3.  Modifying earlier binary classification problem for multi-class problem\n",
    "4.  Implementation and optimization of MNIST classifier.\n",
    "\n",
    "##Pre Reading\n",
    "\n",
    "https://en.wikipedia.org/wiki/MNIST_database\n",
    "\n",
    "https://www.youtube.com/watch?v=S75EdAcXHKk - youtube video introducing some of the nets you'll see.\n",
    "\n",
    "https://github.com/Newmu/Theano-Tutorials - This is the code from the video above.  The code you'll see in class is adapted from the code in this github repo.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Softmax_function\n",
    "\n",
    "http://deeplearning.net/software/theano/library/tensor/nnet/nnet.html#tensor.nnet.categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##What is Back Propagation?\n",
    "\n",
    "Previous lectures have talked about using gradient descent to iteratively improve the weights in machine learning models (i.e. train the models).  With neural nets you will hear the terms \"back propagation\" used in conjunction with weight training.  Backpropagation is basically a bookeeping procedure for efficiently calculating the gradient of error in the network relative to the various weights involved in the neural network.  Backpropagation means calculating the gradient required for gradient descent.  Since you are using Theano for this class, you won't really be required to grind out the details of doing the gradient calculations.  The Theano grad() function will do that for you.  But you will be more comfortable using the system if you understand the basics of backpropagation.  Here is a simple illustration that you can generalize to more realistic cases if required.     \n",
    "\n",
    "The figure below shows an extrememly simple neural network.  It's a three layer network with input, hidden and output layers.  It has a single neuron in each layer.  The activation functions at each layer and the function for measuring the error between prediction and true labels are left as general functions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/simpleNN4BkProp.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here are the definitions and equations that go along with the Figure above.  \n",
    "\n",
    "$x, y$ - input and associated label\n",
    "\n",
    "$w_i$ - weight on input\n",
    "\n",
    "$a_h()$ - activation function in first hidden layer\n",
    "\n",
    "$z_h$ - output of first hidden layer\n",
    "\n",
    "$w_h$ - weight on hidden layer output\n",
    "\n",
    "$a_o$ - output layer activation function\n",
    "\n",
    "$z_o$ - network output\n",
    "\n",
    "$erf(z_o, y)$ - error function for output and associated label\n",
    "\n",
    "To perform gradient descent requires the derivatives of the output error with respect to the weight on the input and the weight on the hidden layer output.  What are the equations linking those?  \n",
    "\n",
    "$z_h = a_h(w_ix)$\n",
    "\n",
    "$z_o = a_o(w_hz_h)$\n",
    "\n",
    "The steps through the network represent function composition that has a certain rythm - multiply by a weight, apply activation function, multiply by a weight, apply activation function, repeat.  The repetitions are function composition with the linear function (multiply by a weight) and function composition with the activation function.  Remember that taking the derivative of a composition of functions results in the products of the derivatives of the functions.  So\n",
    "\n",
    "$\\frac{derf}{dw_h} = \\frac{derf}{dz_o}\\frac{dz_o}{dw_h}$\n",
    "\n",
    "$ = \\frac{derf}{dz_o}\\frac{da_o}{d.}z_h$\n",
    "\n",
    "and look at the relationship between the expression for  $\\frac{derf}{dw_h}$ and $\\frac{derf}{dw_i}$.\n",
    "\n",
    "$\\frac{derf}{dw_i} = \\frac{derf}{dz_o}\\frac{da_o}{d.}\\frac{dz_h}{dw_i}$\n",
    "\n",
    "$= \\frac{derf}{dz_o}\\frac{da_o}{d.}\\frac{da_h}{d.}x$\n",
    "\n",
    "The expression for $\\frac{derf}{dw_i}$ begins with the same terms as the expression for $\\frac{derf}{dw_h}$.  So it makes sense to compute the derivatives starting at the output layer and working backwards through the network.  That saves recomputing the leading terms in the expression.  That's the essence of backpropagation.  \n",
    "\n",
    "Q's\n",
    "1.  Suppose that $a_o$ and $a_h$ are both sigmoid functions ($\\frac{1}{(1 + e^{-x})}$)and that the error is the sum squared error.  Use the expressions above to derive the gradient descent equations.  Notice that \"sum squared error\" implies summing over all the training examples, whereas the equations above, give the gradient contribution from a single training example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##MNIST Image Data Set\n",
    "You will see a lot references to the MNIST image data set and you will see the data set used here as examples for image classification.  The MNIST data have played an important role in the progress of neural net technology and are still used as a standard against which new technologies are tested and benchmarked.  MNIST stands for mixed National Institute of Standards and Technology.  It consists of pixelated handwritten digits 0 - 9.  There are 60000 training examples and 10000 test examples.  Each example in the image data consists of 784 real numbers (28 x 28 pixel values) between 0.0 and 1.0 and a label (0 - 9).  The code below reads in the MNIST data and prints the 784 pixel values for the first example from the training set as a 28 x 28 image.  It also prints out the label in one-hot format and the maximum and minimum values from the first example.  The image shows grey-scale values where 1 gets mapped to white and 0 gets mapped to black.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "__author__ = 'mike.bowles'\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "\n",
    "def one_hot(x, n):\n",
    "    if type(x) == list:\n",
    "        x = np.array(x)\n",
    "    x = x.flatten()\n",
    "    o_h = np.zeros((len(x), n))\n",
    "    o_h[np.arange(len(x)), x] = 1\n",
    "    return o_h\n",
    "\n",
    "\n",
    "def mnist(ntrain=60000, ntest=10000, onehot=True):\n",
    "    fd = open('train-images.idx3-ubyte')\n",
    "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "    trX = loaded[16:].reshape((60000, 28 * 28)).astype(float)\n",
    "    \n",
    "    fd = open('train-labels.idx1-ubyte')\n",
    "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "    trY = loaded[8:].reshape((60000))\n",
    "    \n",
    "    fd = open('t10k-images.idx3-ubyte')\n",
    "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "    teX = loaded[16:].reshape((10000, 28 * 28)).astype(float)\n",
    "    \n",
    "    fd = open('t10k-labels.idx1-ubyte')\n",
    "    loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "    teY = loaded[8:].reshape((10000))\n",
    "\n",
    "    trX = trX / 255.\n",
    "    teX = teX / 255.\n",
    "\n",
    "    trX = trX[:ntrain]\n",
    "    trY = trY[:ntrain]\n",
    "\n",
    "    teX = teX[:ntest]\n",
    "    teY = teY[:ntest]\n",
    "\n",
    "    if onehot:\n",
    "        trY = one_hot(trY, 10)\n",
    "        teY = one_hot(teY, 10)\n",
    "    else:\n",
    "        trY = np.asarray(trY)\n",
    "        teY = np.asarray(teY)\n",
    "\n",
    "    return trX, teX, trY, teY\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = mnist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAD8CAYAAABTq8lnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhCB8INAiBOEDgRYhCB8ItAhB+ECgRfj/bHOL9SyLprsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f31708b1a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-hot label =  [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "max value =  1.0   min value =  0.0\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "plt.imshow(xTrain[idx].reshape(28,28), cmap = cm.Greys_r)\n",
    "plt.show()\n",
    "print 'one-hot label = ', yTrain[idx]\n",
    "print 'max value = ', max(xTrain[idx]), '  min value = ', min(xTrain[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##Q's\n",
    "\n",
    "1.  Pick some different index values in the training set and look at the character image and label to get a feel for the MNIST data set.\n",
    "2.  Rewrite the code snip to select the test image data and labels and look at a few of those examples. \n",
    "\n",
    "\n",
    "##Using Simple two-layer ANN for Classifying MNIST Data\n",
    "In an earlier lecture you saw a simple neural net used for two-class classification.  In this section, you will see very similar code used to classify the MNIST data.  Below is theano code for a two-layer fully connected network.  It consists of an input layer and an output layer.  Instead of a single output neuron like you saw in the two-class classifier, the MNIST classifier has 10 output neurons.  One neuron to predict each of the digits 0 - 9.  The code below implements sigmoid activation functions for these output neurons and then passes all 10 through a softmax function.  See the wikipedia page for a description of the softmax function.  \n",
    "\n",
    "https://en.wikipedia.org/wiki/Softmax_function\n",
    "\n",
    "The softmax function normalizes the values from the 10 output neurons so that they have the properties of a discrete probability distribution - they are all positive numbers and they sum to 1.  Categorical cross entropy is then used to quantify the difference between predicted and actual values.  The one-hot coding for the labels turns the labels into a categorical probability density - one where all the probability mass is concentrated on the label value.  These two probabilities can be compared using cross entropy.  Here's a description of the Theano function for making this comparison. \n",
    "\n",
    "http://deeplearning.net/software/theano/library/tensor/nnet/nnet.html#tensor.nnet.categorical_crossentropy\n",
    "\n",
    "Notice that cross entropy is not a symmetric function of the two probability functions that go into the calculation.  Order matters in the arguments to the function.  \n",
    "\n",
    "Realization of gradient descent has a couple of differences from the simple example introduced earlier.  One is that this realizaiton does not wait to update the weights until it has run through all of the training examples.  Instead it performs weight updates on what are called \"minibatchs\" of 128 training examples.  That speeds up the training process.  The minibatch sizes may require some tuning.  Another difference in this realization, is that it is measuring misclassification error on test data - data that were not included in the training examples.  The printout of the training progress is tracking the percentage of examples that are correctly classified.  It's monitoring a different measure of system performance than is being used in the training.  That's because cross entropy is an analytic function that's differentiable and can be used for gradient descent, whereas misclassification error cannot.  On the other hand, misclassification error is a more intuitive measure of classification performance than cross entropy.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9056\n",
      "10 0.9211\n",
      "20 0.9228\n",
      "30 0.9237\n",
      "40 0.9239\n",
      "50 0.9237\n",
      "60 0.9236\n",
      "70 0.9235\n",
      "80 0.9235\n",
      "90 0.9236\n",
      "100 0.9236\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "from theano import tensor as T\n",
    "import numpy as np\n",
    "from mnistReader import mnist\n",
    "from math import sqrt\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    (h, w) = shape\n",
    "    normalizer = 2.0 * sqrt(6) / sqrt(h + w) * 0.1  #factors: correct for uni[0,1], glo, glo, softmax deriv\n",
    "    #return theano.shared(floatX(np.random.randn(*shape) * 0.01))  #code for standard initialization\n",
    "    return theano.shared(floatX((np.random.random_sample(shape) - 0.5) * normalizer))  #code for using Glorot init\n",
    "    \n",
    "def model(X, w):\n",
    "    return T.nnet.softmax(T.dot(X, w))\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = mnist()\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "w = init_weights((784, 10))\n",
    "\n",
    "py_x = model(X, w)\n",
    "y_pred = T.argmax(py_x, axis=1)\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(py_x, Y))\n",
    "gradient = T.grad(cost=cost, wrt=w)\n",
    "update = [[w, w - gradient * 0.2]]\n",
    "\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
    "\n",
    "for i in range(101):\n",
    "    for start, end in zip(range(0, len(xTrain), 128), range(128, len(xTrain), 128)):\n",
    "        cost = train(xTrain[start:end], yTrain[start:end])\n",
    "    if i % 10 == 0: print i, np.mean(np.argmax(yTest, axis=1) == predict(xTest))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##Coding Exercise\n",
    "\n",
    "1. Implement AdaGrad in place of ordinary gradient descent.  How does that affect the training speed and achievable network performance?\n",
    "\n",
    "For AdaGrad, AdaDelta, NAG methods you'll need to add some more variables to the update equation.  For example the update rule for AdaGrad at the $n^{th}$ step is:\n",
    "\n",
    "$\\Delta w_n = - \\frac{\\eta}{\\sqrt{\\sum_{i = 1}^{n} g_i^2}} g_n$\n",
    "\n",
    "In this equation $\\eta$ is a step size parameter that applies to all the elements of the gradient vector g.  Nothice that the sum of squared past gradients also adjusts the size of the gradient step making the steps smaller as the $g^2$ are accumulated.  But $g^2$ is a vector meaning that different components of the gradient g get different multipliers.  One view of this is that the components that are dominant at the beginning of training are incorporated and then those elements are suppressed allowing for smaller effects to be accumulated.\n",
    "\n",
    "In order to use AdaGrad, you'll need to accumulate the sum of $g_n^2$.  To do that form a new variable - call it sumG, for example.  Then sumG has a theano update that looks like\n",
    "\n",
    "$[(sumG, sumG + g^2)]$\n",
    "\n",
    "This now delivers better numbers than plain gradient descent.  It required some adjustments in order to get performance.  One adjustment was starting the sumG with a small positive number instead of zeros.  The inversion seemed be causing the iteration to diverge.  Another possibility is that the weights aren't being initialized with the Glorot initialization and could be resulting in some saturation leading to very very small gradient values and correspondingly large inverses.  Try changing that to see if it leads to improvement.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9155\n",
      "10 0.9254\n",
      "20 0.9259\n",
      "30 0.9262\n",
      "40 0.9268\n",
      "50 0.927\n",
      "60 0.9272\n",
      "70 0.9268\n",
      "80 0.9269\n",
      "90 0.9264\n",
      "100 0.9264\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "from theano import tensor as T\n",
    "import numpy as np\n",
    "from mnistReader import mnist\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    (h, w) = shape\n",
    "    normalizer = 2.0 * sqrt(6) / sqrt(h + w) * 0.1  #factors: correct for uni[0,1], glo, glo, softmax deriv\n",
    "    #return theano.shared(floatX(np.random.randn(*shape) * 0.01))  #code for standard initialization\n",
    "    return theano.shared(floatX((np.random.random_sample(shape) - 0.5) * normalizer))  #code for using Glorot init\n",
    "    \n",
    "\n",
    "def model(X, w):\n",
    "    return T.nnet.softmax(T.dot(X, w))\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = mnist()\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "w = init_weights((784, 10))\n",
    "sumG = theano.shared(np.zeros((784, 10), dtype=theano.config.floatX) + 0.0001)\n",
    "eta = theano.shared(np.array(0.05, dtype=theano.config.floatX))\n",
    "\n",
    "py_x = model(X, w)\n",
    "y_pred = T.argmax(py_x, axis=1)\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(py_x, Y))\n",
    "gradient = T.grad(cost=cost, wrt=w)\n",
    "newSumG = sumG + gradient * gradient\n",
    "rootGSq = T.sqrt(newSumG)\n",
    "update = [[w, w - eta * gradient / rootGSq], [sumG, sumG + gradient * gradient]]\n",
    "\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
    "\n",
    "for i in range(101):\n",
    "    for start, end in zip(range(0, len(xTrain), 128), range(128, len(xTrain), 128)):\n",
    "        cost = train(xTrain[start:end], yTrain[start:end])\n",
    "    if i % 10 == 0: print i, np.mean(np.argmax(yTest, axis=1) == predict(xTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Discussion of training improvements in Glorot, Bengio paper\n",
    "\n",
    "The paper that you saw in last lecture highlighted the problems of training multilayer networks and gave several recommendations describing how to avoid these problems.  \n",
    "\n",
    "http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\n",
    "\n",
    "##Q's\n",
    "\n",
    "1.  What does it mean to say that a neuron with sigmoid activation function is saturated?\n",
    "\n",
    "2.  Which layers in a deep network are most prone to saturate?\n",
    "\n",
    "3.  What properties of the weight initialization lead to saturation?\n",
    "\n",
    "4.  What problems do the authors have with sigmoid non-linearity?\n",
    "\n",
    "##Coding Homework\n",
    "\n",
    "The Glorot, Bengio paper suggests that hyperbolic tangent avoids bias issues inherent using the sigmoid function for activation.  It also gives specific numeric recommendations regarding the initialization values for the weights.  Implement one or the other or both of these changes, in addition to using one of the acceleration methods (AdaGrad, AdaDelt, Nesterov, Adam).  How does that affect training speed and network performance.  (Need some volunteers to take various combinations in order to compare.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
