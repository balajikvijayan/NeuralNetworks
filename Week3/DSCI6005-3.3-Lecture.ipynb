{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Traditional (10 years ago) 3-layer ANN\n",
    "\n",
    "##What you'll learn in this lecture\n",
    "1.  How to add momentum-method weight update to simple ANN\n",
    "2.  The structure of a standard neural net\n",
    "3.  Familarization with Cifar data set\n",
    "4.  How to code a simple ANN for color images\n",
    "\n",
    "##Order of topics\n",
    "\n",
    "1.  Upgrade 2-layer ANN to incorporate momentum.\n",
    "2.  Architecture of standard neural net\n",
    "3.  Code for MNIST classifier using standard ANN\n",
    "4.  Introduction to Cifar image classification data set\n",
    "5.  Coding exercise to build Cifar classifier\n",
    "\n",
    "\n",
    "##Pre-reading\n",
    "\n",
    "http://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "\n",
    "\n",
    "##Upgrading 2-Layer Network to Incorporate Momentum-Method Weight Updates\n",
    "\n",
    "Recall the equations for momentum method from Lecture 2.2.  They're repeated below.\n",
    "\n",
    "$Step_{i+1} = \\beta Step_i - (1 - \\beta)\\delta\\nabla J_i$\n",
    "\n",
    "and\n",
    "\n",
    "$w_{i+1} = w_i + Step_{i+1}$\n",
    "\n",
    "With simple gradient descent you adjust incrementally with gradient information.  With momentum method you introduce a new variable (called \"step\" in the equation above).  The new variable also gets adjusted incrementally using gradient information and the weights now get adjusted incrementally using the new variable.  Recall that the \"update\" parameter in the theano function provided the mechanism for incrementally adjusting the weights with the gradient.  You'll have to use that same mechanism for updating the new variable that momentum method introduces.  \n",
    "\n",
    "The code block below highlights the alterations required to incorporate momentum method updates.  The key difference shows up in the definition of \"update\".  With pure gradient descent the line of code defining the update previously was:\n",
    "\n",
    "update = [[w, w - gradient * 0.05]]\n",
    "\n",
    "The python list had two components.  The first is the (shared) variable being updated in the function and the second is the new value to be assigned.  If you were doing the updates in python you might author a line something like \n",
    "\n",
    "w = w - gradient * 0.05.  \n",
    "\n",
    "The old definition gets replaced with \n",
    "\n",
    "update = [(w, w - dw), (dw, beta * dw + (1 - beta) * delta * gradient)].  \n",
    "\n",
    "Inside the list there are two tuples.  What's called \"step\" in the equation above is called \"dw\" in the code.  The first tuple is (w, w - dw) which describes the new weight update and the second one is (dw, beta * dw + (1 - beta) * delta * gradient) which describes the update for the new variable introduced by the momentum method.  Enclosing the update expressions in a tuple instead a list demonstrates that the tuple works in the same way.  You can use whichever is convenient for you.  The second novel element is that the list of variables that require updating leads to a list of tuples (or lists) each tuple describes the update equation for its corresponding variable.  You'll see that mechanism exercised again to update weights for more than one layer.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9046\n",
      "10 0.9225\n",
      "20 0.9233\n",
      "30 0.9239\n",
      "40 0.9232\n",
      "50 0.9223\n",
      "60 0.9211\n",
      "70 0.9213\n",
      "80 0.9215\n",
      "90 0.9214\n",
      "100 0.9218\n"
     ]
    }
   ],
   "source": [
    "#code for 2-layer classifier network\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "__author__ = 'mike.bowles'\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import matplotlib.cm as cm\n",
    "from mnistReader import mnist\n",
    "%matplotlib inline\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    return theano.shared(floatX(np.random.randn(*shape) * 0.01))\n",
    "\n",
    "def init_dw(shape):                                #new init function for momentum variable\n",
    "    return theano.shared(floatX(np.zeros(shape)))\n",
    "\n",
    "def model(X, w):\n",
    "    return T.nnet.softmax(T.dot(X, w))\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = mnist()\n",
    "\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "shape = (784, 10)\n",
    "w = init_weights(shape)\n",
    "dw = init_dw(shape)\n",
    "\n",
    "\n",
    "py_x = model(X, w)\n",
    "y_pred = T.argmax(py_x, axis=1)\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(py_x, Y))\n",
    "gradient = T.grad(cost=cost, wrt=w)\n",
    "\n",
    "beta = T.constant(0.9)    #parameter for momentum method\n",
    "delta = T.constant(0.5)   #parameter for momentum method\n",
    "\n",
    "update = [(w, w + dw), (dw, beta * dw - (1 - beta) * delta * gradient)]  #expanded for momentum\n",
    "\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
    "\n",
    "for i in range(101):\n",
    "    for start, end in zip(range(0, len(xTrain), 128), range(128, len(xTrain), 128)):\n",
    "        cost = train(xTrain[start:end], yTrain[start:end])\n",
    "    if i%10 == 0: print i, np.mean(np.argmax(yTest, axis=1) == predict(xTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##In-class coding exercise\n",
    "1.  In the 2-layer MNIST classifier change the weights to satisfy Glorot's conditions and change the weight updates from momentum to AdaGrad or AdaDelta.  \n",
    "\n",
    "##The ANN BD (Before Deep Learning)\n",
    "\n",
    "The figure below shows the basic structure used for neural up until new training approaches were developed in the last 10 years or so.  The network has three layers - input layer, hidden layer and output layer.  Typically the number of hidden neurons is less than the number of input neurons and the number of output neurons is smaller still (dictated largely by the type of problem being solved).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  0   misclassification error rate =  0.7021\n",
      "i =  10   misclassification error rate =  0.909\n",
      "i =  20   misclassification error rate =  0.9184\n",
      "i =  30   misclassification error rate =  0.9227\n",
      "i =  40   misclassification error rate =  0.9282\n",
      "i =  50   misclassification error rate =  0.9338\n",
      "i =  60   misclassification error rate =  0.9397\n",
      "i =  70   misclassification error rate =  0.945\n",
      "i =  80   misclassification error rate =  0.9491\n",
      "i =  90   misclassification error rate =  0.9536\n",
      "i =  100   misclassification error rate =  0.9574\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "from theano import tensor as T\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "from scipy.misc import imsave\n",
    "\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    (h, w) = shape\n",
    "    normalizer = 2.0 * sqrt(6) / sqrt(h + w) * 0.2 # 0.25 for sigmoid and 0.1 for softmax\n",
    "    return theano.shared(floatX(np.random.randn(*shape) * 0.01))  #code for standard initialization\n",
    "    #return theano.shared(floatX((np.random.random_sample(shape) - 0.5) * normalizer))  #code for using Glorot init\n",
    "\n",
    "def sgd(cost, params, lr=0.05):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        updates.append([p, p - g * lr])\n",
    "    return updates\n",
    "\n",
    "def model(X, w_h, w_o):\n",
    "    #code of tahn activation functions\n",
    "    #h = T.tanh(T.dot(X, w_h))\n",
    "\n",
    "    #code for sigmoid activation functions\n",
    "    h = T.nnet.sigmoid(T.dot(X, w_h))\n",
    "    pyx = T.nnet.softmax(T.dot(h, w_o))\n",
    "    return pyx\n",
    "\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "w_h = init_weights((784, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "\n",
    "py_x = model(X, w_h, w_o)\n",
    "y_x = T.argmax(py_x, axis=1)\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(py_x, Y))\n",
    "params = [w_h, w_o]\n",
    "updates = sgd(cost, params)\n",
    "\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True)\n",
    "\n",
    "for i in range(101):\n",
    "    for start, end in zip(range(0, len(xTrain), 128), range(128, len(xTrain), 128)):\n",
    "        cost = train(xTrain[start:end], yTrain[start:end])\n",
    "    if i%10 == 0: print \"i = \", i, \"  misclassification error rate = \", np.mean(np.argmax(yTest, axis=1) == predict(xTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##In-Class coding exercise\n",
    "\n",
    "1.  Change to Glorot, Bengio initial weights and tanh non-linearities and rerun.  You can make those changes by  uncommenting the code above\n",
    "\n",
    "2.  Pick a configuration and switch to AdaGrad, AdaDelta or NAG updates (keeping Glorot, Bengio initial wts).  What is the difference?\n",
    "\n",
    "##CIFAR data set - processing full color images\n",
    "\n",
    "Here's a link to the web page where you can download the Cifar image classification images.  You'll build a neural net to classifiy the Cifar-10 images.  This data set has 60,000 32x32 pixel images containing 10 different classes.  The data set is balanced.  It contains 6000 of each of the 10 classes.  The data are divided into 50,000 training examples and 10,000 test examples.  The 10 classes are - airplane, automobile, bird, cat, deer, dog, frog, horse, ship and truck.  The image below shows several examples.  \n",
    "<img src=\"CifarImages.png\">\n",
    "\n",
    "The code below reads the training and test data sets.  You'll only train on 10,000 of the training examples in order to cut down on the training time.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xTrain shape =  (10000, 3072)\n",
      "yTrain shape =  (10000, 10)\n",
      "xTest shape =  (10000, 3072)\n",
      "yTest shape =  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "__author__ = 'mike.bowles'\n",
    "from readCifar import cifar\n",
    "\n",
    "xTrain, yTrain, xTest, yTest = cifar()\n",
    "\n",
    "print 'xTrain shape = ', xTrain.shape\n",
    "print 'yTrain shape = ', yTrain.shape\n",
    "print 'xTest shape = ', xTest.shape\n",
    "print 'yTest shape = ', yTest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Q's\n",
    "\n",
    "1.  There are 10000 rows of x's and y's for training and for testing.  Explain the other dimension of the xTrain and xTest arrays.  \n",
    "\n",
    "##In-class coding exercise\n",
    "\n",
    "Adapt the simple logistic regression code from last lecture to train a classifier for the cifar images.  Notice that the code snip above has already read in the training and testing files.  The code block below contains the MNIST classifier code from last lecture as a starting point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "from theano import tensor as T\n",
    "import numpy as np\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    return theano.shared(floatX(np.random.randn(*shape) * 0.01))\n",
    "\n",
    "def model(X, w):\n",
    "    return T.nnet.softmax(T.dot(X, w))\n",
    "\n",
    "xTrain, yTrain, xTest, yTest = mnist(onehot=True)\n",
    "\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "w = init_weights((784, 10))\n",
    "\n",
    "py_x = model(X, w)\n",
    "y_pred = T.argmax(py_x, axis=1)\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(py_x, Y))\n",
    "gradient = T.grad(cost=cost, wrt=w)\n",
    "update = [[w, w - gradient * 0.05]]\n",
    "\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
    "\n",
    "for i in range(101):\n",
    "    for start, end in zip(range(0, len(trX), 128), range(128, len(trX), 128)):\n",
    "        cost = train(trX[start:end], trY[start:end])\n",
    "    if i % 10 == 0: print i, np.mean(np.argmax(teY, axis=1) == predict(teX))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Homework exercise\n",
    "Code a standard 3 layer ANN for classifying Cifar images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
