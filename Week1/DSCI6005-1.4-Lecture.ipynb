{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Other High Performance Optimization Methods\n",
    "\n",
    "Topics for Today\n",
    "1.  Conjugate gradient algorithm for minimizing differentiable function.\n",
    "2.  BGFS algorithm \n",
    "\n",
    "What you will learn\n",
    "1. What are quasi-Newton methods.\n",
    "2. How to use conjugate gradient and BGFS to train some machine learning algorithms.\n",
    "\n",
    "\n",
    "The optimization methods you saw in the last lecture are general purpose and can be used for a variety of applications.  They were designed with ANN training in mind and are well suited for problems with a large number of degrees of freedom.  Numerical optimization has been studied for a number of years and has yielded a number of algorithms that you may find useful.  Here are a couple of them.  \n",
    "\n",
    "#Conjugate gradient\n",
    "\n",
    "Conjugate gradient was originally developed for solving systems of linear equations (i.e. matrix equations) that were positive definite - like the quadratic form problem you've seen used to demonstrate other optimization techniques.  The wiki pages on conjugate gradient and non-linear conjugate gradient give good coverage of how the method works.  Here are links to those.  \n",
    "https://en.wikipedia.org/wiki/Conjugate_gradient_method\n",
    "https://en.wikipedia.org/wiki/Nonlinear_conjugate_gradient_method\n",
    "\n",
    "The code below applies the scipy version of this algoritm to the quadratic form problem that you've seen before.  Watch how convergence changes as you alter the coefficients in the quadratic form.  Try the values that were easy for gradient descent and the ones that gave it problems.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 2\n",
      "         Function evaluations: 16\n",
      "         Gradient evaluations: 4\n",
      "[  9.80097900e-09  -9.80098040e-05]\n"
     ]
    }
   ],
   "source": [
    "__author__ = 'mike.bowles'\n",
    "import numpy as np\n",
    "from scipy import optimize\n",
    "\n",
    "aList = [100.0, 1.0]\n",
    "dum = 1.0\n",
    "args = (dum, aList)   #diagonal entries in QF\n",
    "\n",
    "def J(w, *args):\n",
    "    w1, w2 = w\n",
    "    (dum2, aList2) = args\n",
    "    return 0.5 * (aList2[0] * w1 * w1 + aList[1] * w2 * w2)\n",
    "\n",
    "def gradJ(w, *args):\n",
    "    w1, w2 = w\n",
    "    (dum2, aList2) = args\n",
    "    return np.asarray((aList2[0] * w1, aList2[1] * w2))\n",
    "\n",
    "x0 = np.asarray((1.0, 1.0))\n",
    "\n",
    "wStar = optimize.fmin_cg(J, x0, fprime=gradJ, args=args)\n",
    "\n",
    "print wStar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class programming exercise\n",
    "1.  Use conjugate gradient to find weights for the ANN classifier that we've been using as an example. \n",
    "\n",
    "#BFGS\n",
    "BFGS is one of a class of optimization techniques called \"quasi-Newton\" methods.  Quasi-Newton methods only use the gradient of the function being minimized.  They are first order methods.  But they attempt to construct and approximate Hessian matrix.  This gives quasi-Newton methods improved convergence speed relative to gradient descent.  \n",
    "\n",
    "The wiki page on BFGS has a good explanation of the algorithm.  Here's the link: https://en.wikipedia.org/wiki/Broyden–Fletcher–Goldfarb–Shanno_algorithm\n",
    "\n",
    "The code below shows the application of BFGS to the simple quadratic form.  Experiment with different values for the matrix in the quadratic form to see how the convergence of BFGS is affected.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ -1.04913335e-11,  -6.99794204e-10]), 2.5035936752922889e-19, {'warnflag': 0, 'task': 'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL', 'grad': array([ -1.04913335e-09,  -6.99794204e-10]), 'nit': 5, 'funcalls': 8})\n"
     ]
    }
   ],
   "source": [
    "__author__ = 'mike.bowles'\n",
    "import numpy as np\n",
    "from scipy import optimize\n",
    "\n",
    "aList = [100.0, 1.0] #diagonal entries in QF\n",
    "args = (aList)   \n",
    "\n",
    "def J(w, *args):\n",
    "    w1, w2 = w\n",
    "    [a11, a22] = args\n",
    "    return 0.5 * (a11 * w1 * w1 + a22 * w2 * w2)\n",
    "\n",
    "def gradJ(w, *args):\n",
    "    w1, w2 = w\n",
    "    [a11, a22] = args\n",
    "    return np.asarray((a11 * w1, a22 * w2))\n",
    "\n",
    "x0 = np.asarray((1.0, 1.0))\n",
    "\n",
    "wStar = optimize.fmin_l_bfgs_b(J, x0, fprime=gradJ, args=args)\n",
    "\n",
    "print wStar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#In-class exercise\n",
    "2.  Use BFGS to find weights for the ANN classifier that we've been using as an example.  As you've seen in the illustrative example changing from the algorithm you used in exercise 1 is quick and easy.  \n",
    "\n",
    "Comment on differences in behavior between conjugate gradient and BFGS.  \n",
    "\n",
    "What you've learned\n",
    "1.  How to use conjugate gradient and BGFS in python to train a neural net.  \n",
    "2.  Comparison between these algorithms and others you've seen. \n",
    "\n",
    "\n",
    "#Homework:\n",
    "1.  Use minimization algorithms to solve for regression coefficients. The code snippet below generates data for a regression problem (i.e. a problem where the outcome is a real number).  Uncomment the print statement to have a look at the data.  You can see from the way the data are generated that the best estimate of the outcome (Y) is 10*x1 + x2.  See if you can generate that answer by finding the coefficients to minimize the sum squared error (SSE).  \n",
    "\n",
    "$SSE = \\frac{1.0}{M} \\sum\\limits_{i=0}^{M-1} (y(i) - (b + w1*x1(i) + w2*x2(i))^2$\n",
    "\n",
    "In this equation M is the number of examples in the data set (200 in the code snippet).  There are three parameters that you need to determine in order to have a solution - b, w1, and w2.  First develop the expressions for the gradient of the summands relative to each of these.  Notice that multiplying the SSE by any constants (like $\\frac1M$ or $frac12$) doesn't change where the minimum occurs so feel free to multiply by whatever constants make the resulting expressions simpler or easier to interpret.  Once you have the expressions for the three elements of the gradient vector, you can formulate the gradient descent problem and solve it using at least three of the minimization techniques that we've covered.  Use plain vanilla gradient descent, one of the four accelerated gradient descent methods (NAG, AdaGrad, AdaDelta or Adam) and one of conjugate gradient or BGFS.  Comment on similarities and differences between these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = 'mike.bowles'\n",
    "\n",
    "import random\n",
    "\n",
    "X1 = []\n",
    "X2 = []\n",
    "Y = []\n",
    "noiseSd = 0.2\n",
    "numRows = 200\n",
    "for i in range(numRows):\n",
    "    #generate attributes x1 and x2 by drawing from uniform (0,1)\n",
    "    x1 = random.random()\n",
    "    x2 = 10.0 * random.random()\n",
    "    y = x1 + x2 + random.normalvariate(0.0, noiseSd)\n",
    "    X1.append(x1); X2.append(x2); Y.append(y);\n",
    "\n",
    "    #print x1, x2, y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
