{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Lecture 5.1 - Auto Encoders and Semi-Supervised Learning\n",
    "\n",
    "##Learning Objectives\n",
    "1.  What is semi-supervised learning?\n",
    "2.  What is an auto encoder?\n",
    "3.  How can an autoencoder be built for semi-supervised learning problems?\n",
    "\n",
    "##Order of Topics\n",
    "1.  Definition of semi-supervised problems and some examples\n",
    "2.  Background on stacked autoencoders\n",
    "3.  Walkthrough code for 2-hidden-layer autoencoder.\n",
    "\n",
    "\n",
    "##Pre-class reading\n",
    "https://en.wikipedia.org/wiki/Semi-supervised_learning - Background on semi-supervised learning problems\n",
    "\n",
    "http://ufldl.stanford.edu/wiki/index.php/Stacked_Autoencoders - Look this over.  We'll build one of these autoencoders in theano\n",
    "\n",
    "##Semi-supervised learning\n",
    "Sometimes label are easy to produce along with each instance.  For example, if the problem predict whether a visitor to a web site will click on an ad, then the labels for a training set are whether or not past customers clicked and the cost of labeling the data is just the cost of storage for the \"yes/no\" bit.  A computer will do the work of attaching labels to each example.  If the problem is to determine whether a picture violates the standards of your website, then the images are relatively easy to get, but labeling them requires a human looking at all the pictures.  Maybe that's a job for mechanical turk.  If the problem is determining the toxicity of a drug from giving the drug to rats and then inspecting their livers for signs of damage, then getting the examples takes time and money and the associated labels require a reading from a pathologist.  Semi-supervised learning is a collection of techniques for incorporating both labeled and unlabeled data in order to achieve better accuracy than would be available with either supervised or unsupervised techniques alone.  You'll see a couple of methods for accomplish this.  Today you'll see how to use some of the deep learning techniques that you've learning in the last couple of weeks.  \n",
    "\n",
    "##Autoencoder stack\n",
    "The basic idea with an autoencoder stack is to train a neural network one layer at a time, without using labels.  Instead of training against labels, each layer is paired with a symmetric output layer and trained that stack is trained to reproduce its input.  Walking through the figures included in http://ufldl.stanford.edu/wiki/index.php/Stacked_Autoencoders will make the concepts clear.  \n",
    "\n",
    "The code below demonstrates training the first layer in the stack and shows the results of training.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = 'mike_bowles'\n",
    "#training for 1st layer\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from mnReader import mnist\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from random import sample\n",
    "\n",
    "srng = RandomStreams()\n",
    "\n",
    "\n",
    "def writeFile(w, filename):\n",
    "    import cPickle\n",
    "    import os\n",
    "    file = open(filename, 'wb')\n",
    "    cPickle.dump(w, file)\n",
    "\n",
    "def readFile(filename):\n",
    "    import cPickle\n",
    "    import os\n",
    "    file = open(filename, 'rb')\n",
    "    return cPickle.load(file)\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    (h, w) = shape\n",
    "    # Glorot normalization - last factor depends on non-linearity\n",
    "    # 0.25 for sigmoid and 0.1 for softmax, 1.0 for tanh or Relu\n",
    "    normalizer = 2.0 * sqrt(6) / sqrt(h + w) * 1.0\n",
    "    #return theano.shared(floatX(np.random.randn(*shape) * 0.01))  #code for standard initialization\n",
    "    #code for using Glorot initialization\n",
    "    return theano.shared(floatX((np.random.random_sample(shape) - 0.5) * normalizer))\n",
    "\n",
    "def rectify(X):\n",
    "    #return T.maximum(X, 0.)   #original rectifier\n",
    "    return T.maximum(X, 0.01 * X)  #leaky rectifier\n",
    "\n",
    "\n",
    "def RMSprop(cost, params, lr=0.001, rho=0.9, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - lr * g))\n",
    "    return updates\n",
    "\n",
    "def adaGrad(cost, params, eta=0.1, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        sumGSq = theano.shared(p.get_value() * 0.)\n",
    "        sumGSq_new = sumGSq + g ** 2\n",
    "        gradient_scaling = T.sqrt(sumGSq_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((sumGSq, sumGSq_new))\n",
    "        updates.append((p, p - eta * g))\n",
    "    return updates\n",
    "\n",
    "def adaDelta(cost, params, eta=1.0, rho=0.9, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        #calc g-squared\n",
    "        gSq = theano.shared(p.get_value() * 0.)\n",
    "        dwSq = theano.shared(p.get_value() * 0.)\n",
    "\n",
    "        #exp smoothed g squared\n",
    "        gSqNew = rho * gSq + (1 - rho) * g * g\n",
    "\n",
    "        #calc dx-squared\n",
    "        dw = eta * T.sqrt(dwSq + epsilon) * g / T.sqrt(gSq + epsilon)\n",
    "        dwSqNew = rho * dwSq + (1 - rho) * dw * dw\n",
    "\n",
    "        updates.append((dwSq, dwSqNew))\n",
    "        updates.append((gSq, gSqNew))\n",
    "        updates.append((p, p - dw))\n",
    "    return updates\n",
    "\n",
    "def nesterovAG(cost, params, c=1.0):\n",
    "    grads = T.grad(cost, wrt=params)\n",
    "    updates = []\n",
    "    lam = theano.shared(np.asarray(1.0, dtype=theano.config.floatX), 'lam')\n",
    "    lamSP1 = (1 + T.sqrt(1 + 4 * lam * lam)) / 2.0\n",
    "    updates.append((lam, lamSP1))\n",
    "    gammaS = (1 - lam) / lamSP1\n",
    "\n",
    "    for p,g in zip(params, grads):\n",
    "        #calc y and x at s + 1\n",
    "        yS = theano.shared(p.get_value())  #initialize yS\n",
    "        ySP1 = p - g / c\n",
    "        pSP1 = (1 - gammaS) * ySP1 + gammaS * yS\n",
    "\n",
    "        updates.append((yS, ySP1))\n",
    "        updates.append((p, pSP1))\n",
    "    return updates\n",
    "\n",
    "def dropout(X, p=0.):\n",
    "    if p > 0:\n",
    "        retain_prob = 1 - p\n",
    "        X *= srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n",
    "        X /= retain_prob\n",
    "    return X\n",
    "\n",
    "def model(X, w_h, w_o, p_drop_input, p_drop_hidden):\n",
    "    X = dropout(X, p_drop_input)\n",
    "    h = rectify(T.dot(X, w_h))\n",
    "\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    x_x = rectify(T.dot(h, w_o))\n",
    "\n",
    "    return h, x_x\n",
    "\n",
    "\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "w_h = init_weights((784, 450))\n",
    "w_o = init_weights((450, 784))\n",
    "\n",
    "#Tie weights together\n",
    "wtVal = 0.5 * (w_h.get_value() + np.transpose(w_o.get_value()))\n",
    "w_h.set_value(wtVal)\n",
    "w_o.set_value(np.transpose(wtVal))\n",
    "\n",
    "noise_h, noise_x_x = model(X, w_h, w_o, 0.2, 0.5)\n",
    "h, x_x = model(X, w_h, w_o, 0., 0.)\n",
    "\n",
    "cost = T.mean(T.sqr(X - x_x))\n",
    "params = [w_h, w_o]\n",
    "\n",
    "#updates = RMSprop(cost, params, lr=0.0001)\n",
    "updates = adaGrad(cost, params, eta=0.1, epsilon=1.0) #\n",
    "#updates = adaDelta(cost, params, eta=0.1, rho=0.9, epsilon=1.0e-1)\n",
    "#updates = nesterovAG(cost, params, c=100.0)\n",
    "\n",
    "xTr, xTe, yTr, yTe = mnist(onehot=True)\n",
    "\n",
    "train = theano.function(inputs=[X], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=x_x, allow_input_downcast=True)\n",
    "\n",
    "\n",
    "for i in range(101):\n",
    "    iJump = 40\n",
    "\n",
    "    for start, end in zip(range(0, len(xTr), iJump), range(iJump, len(xTr), iJump)):\n",
    "        cost = train(xTr[start:end])\n",
    "        #make sure weights stay tied together\n",
    "        wtVal = 0.5 * (w_h.get_value() + np.transpose(w_o.get_value()))\n",
    "        w_h.set_value(wtVal)\n",
    "        w_o.set_value(np.transpose(wtVal))\n",
    "    dum, outTr = model(xTr[start:end], w_h, w_o, 0.0, 0.0)\n",
    "    dum, outTe = model(xTe, w_h, w_o, 0.0, 0.0)\n",
    "    print i, np.mean(np.square(xTr[start:end] - outTr.eval())), np.mean(np.square(xTe - outTe.eval()))\n",
    "    filename = 'wh1_784x450'\n",
    "    writeFile(w_h, filename)\n",
    "\n",
    "    #asList = list(itertools.chain.from_iterable((w_h.get_value()).tolist()))\n",
    "    #plt.hist(asList)\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  training error   test error\n",
    "\n",
    "0 0.00720460403344 0.00558570951932  \n",
    "1 0.00543443144029 0.00422348246745  \n",
    "2 0.0046455672724 0.00361716637157  \n",
    "3 0.00417445467595 0.00324217888212  \n",
    "4 0.00385111593976 0.00297388732927  \n",
    "5 0.00360445778003 0.00276614644623  \n",
    "6 0.00340239912557 0.00259675805118  \n",
    "7 0.0032322766274 0.0024537540176  \n",
    "8 0.00308121691577 0.00233017746953  \n",
    "9 0.00295164503737 0.00222138863088  \n",
    "10 0.00283979780605 0.00212492667653  \n",
    "11 0.00273879005775 0.00203863466993  \n",
    "12 0.00264886922637 0.00196052569108  \n",
    "13 0.00256720595295 0.00188942103276  \n",
    "14 0.00249258589244 0.00182407461019  \n",
    "15 0.00242454550107 0.00176386154547  \n",
    "16 0.00236264458621 0.00170793366705  \n",
    "17 0.00230631018995 0.00165582853733  \n",
    "18 0.00225383456126 0.00160708400946  \n",
    "19 0.0022053266137 0.0015612323431  \n",
    "20 0.00215938875221 0.0015178246612  \n",
    "21 0.00211525037984 0.0014767117029  \n",
    "22 0.00207342402015 0.00143746157609  \n",
    "23 0.00203322783804 0.00139999618543  \n",
    "24 0.00199495118219 0.00136412340604  \n",
    "25 0.00195746655757 0.00132977314186  \n",
    "26 0.00192219023513 0.00129674631617  \n",
    "27 0.00188703374476 0.00126498188364  \n",
    "28 0.00185297203411 0.00123442277568  \n",
    "29 0.00182144361564 0.00120480502838  \n",
    "30 0.00179030642091 0.0011763192598  \n",
    "31 0.00175912686576 0.00114904858212  \n",
    "32 0.00172852706855 0.00112287862498  \n",
    "33 0.00169923333915 0.00109772200791  \n",
    "34 0.00167067637625 0.00107358720205  \n",
    "35 0.00164296463865 0.00105040718782  \n",
    "36 0.00161614120953 0.00102812043263  \n",
    "37 0.00158991740138 0.00100669241586  \n",
    "38 0.00156435792518 0.000986102135528  \n",
    "\n",
    "The next section of code takes the 1st layer weights calculated by the program above and uses them to calculate the input to the second hidden layer.  The input to the second layer is then treated as input and labels for training the second layer weights.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 450)\n",
      "(60000, 450) (10000, 450)\n",
      "0 0.0126660570167 0.0114325418772\n",
      "1 0.00962611170861 0.00867053096868\n",
      "2 0.0082047344161 0.0073957540318\n"
     ]
    }
   ],
   "source": [
    "\n",
    "__author__ = 'mike_bowles'\n",
    "#training for second layer\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from mnistReader import mnist\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from random import sample\n",
    "\n",
    "srng = RandomStreams()\n",
    "\n",
    "\n",
    "def writeFile(w, filename):\n",
    "    import cPickle\n",
    "    import os\n",
    "    file = open(filename, 'wb')\n",
    "    cPickle.dump(w, file)\n",
    "\n",
    "def readFile(filename):\n",
    "    import cPickle\n",
    "    import os\n",
    "    file = open(filename, 'rb')\n",
    "    return cPickle.load(file)\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    (h, w) = shape\n",
    "    # Glorot normalization - last factor depends on non-linearity\n",
    "    # 0.25 for sigmoid and 0.1 for softmax, 1.0 for tanh or Relu\n",
    "    normalizer = 2.0 * sqrt(6) / sqrt(h + w) * 1.0\n",
    "    #return theano.shared(floatX(np.random.randn(*shape) * 0.01))  #code for standard initialization\n",
    "    #code for using Glorot initialization\n",
    "    return theano.shared(floatX((np.random.random_sample(shape) - 0.5) * normalizer))\n",
    "\n",
    "def rectify(X):\n",
    "    #return T.maximum(X, 0.)   #original rectifier\n",
    "    return T.maximum(X, 0.01 * X)  #leaky rectifier\n",
    "\n",
    "\n",
    "def RMSprop(cost, params, lr=0.001, rho=0.9, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - lr * g))\n",
    "    return updates\n",
    "\n",
    "def adaGrad(cost, params, eta=0.1, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        sumGSq = theano.shared(p.get_value() * 0.)\n",
    "        sumGSq_new = sumGSq + g ** 2\n",
    "        gradient_scaling = T.sqrt(sumGSq_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((sumGSq, sumGSq_new))\n",
    "        updates.append((p, p - eta * g))\n",
    "    return updates\n",
    "\n",
    "def adaDelta(cost, params, eta=1.0, rho=0.9, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        #calc g-squared\n",
    "        gSq = theano.shared(p.get_value() * 0.)\n",
    "        dwSq = theano.shared(p.get_value() * 0.)\n",
    "\n",
    "        #exp smoothed g squared\n",
    "        gSqNew = rho * gSq + (1 - rho) * g * g\n",
    "\n",
    "        #calc dx-squared\n",
    "        dw = eta * T.sqrt(dwSq + epsilon) * g / T.sqrt(gSq + epsilon)\n",
    "        dwSqNew = rho * dwSq + (1 - rho) * dw * dw\n",
    "\n",
    "        updates.append((dwSq, dwSqNew))\n",
    "        updates.append((gSq, gSqNew))\n",
    "        updates.append((p, p - dw))\n",
    "    return updates\n",
    "\n",
    "def nesterovAG(cost, params, c=1.0):\n",
    "    grads = T.grad(cost, wrt=params)\n",
    "    updates = []\n",
    "    lam = theano.shared(np.asarray(1.0, dtype=theano.config.floatX), 'lam')\n",
    "    lamSP1 = (1 + T.sqrt(1 + 4 * lam * lam)) / 2.0\n",
    "    updates.append((lam, lamSP1))\n",
    "    gammaS = (1 - lam) / lamSP1\n",
    "\n",
    "    for p,g in zip(params, grads):\n",
    "        #calc y and x at s + 1\n",
    "        yS = theano.shared(p.get_value())  #initialize yS\n",
    "        ySP1 = p - g / c\n",
    "        pSP1 = (1 - gammaS) * ySP1 + gammaS * yS\n",
    "\n",
    "        updates.append((yS, ySP1))\n",
    "        updates.append((p, pSP1))\n",
    "    return updates\n",
    "\n",
    "def dropout(X, p=0.):\n",
    "    if p > 0:\n",
    "        retain_prob = 1 - p\n",
    "        X *= srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n",
    "        X /= retain_prob\n",
    "    return X\n",
    "\n",
    "def model(X, w_h, w_o, p_drop_input, p_drop_hidden):\n",
    "    X = dropout(X, p_drop_input)\n",
    "    h = rectify(T.dot(X, w_h))\n",
    "\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    x_x = rectify(T.dot(h, w_o))\n",
    "\n",
    "    return h, x_x\n",
    "\n",
    "\n",
    "X = T.fmatrix()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "w_h = init_weights((450, 225))\n",
    "w_o = init_weights((225, 450))\n",
    "\n",
    "#Tie weights together\n",
    "wtVal = 0.5 * (w_h.get_value() + np.transpose(w_o.get_value()))\n",
    "w_h.set_value(wtVal)\n",
    "w_o.set_value(np.transpose(wtVal))\n",
    "\n",
    "noise_h, noise_x_x = model(X, w_h, w_o, 0.2, 0.5)\n",
    "h, x_x = model(X, w_h, w_o, 0., 0.)\n",
    "\n",
    "cost = T.mean(T.sqr(X - x_x))\n",
    "params = [w_h, w_o]\n",
    "\n",
    "#updates = RMSprop(cost, params, lr=0.0001)\n",
    "updates = adaGrad(cost, params, eta=0.1, epsilon=0.001) #\n",
    "#updates = adaDelta(cost, params, eta=0.1, rho=0.9, epsilon=1.0e-1)\n",
    "#updates = nesterovAG(cost, params, c=100.0)\n",
    "\n",
    "xTrTmp, xTeTmp, yTr, yTe = mnist(onehot=True)\n",
    "w1 = readFile('wh1_784x450-keep')\n",
    "\n",
    "print w1.shape\n",
    "xTr = np.maximum(np.dot(xTrTmp, w1), 0.01 * np.dot(xTrTmp, w1))\n",
    "xTe = np.maximum(np.dot(xTeTmp, w1), 0.01 * np.dot(xTeTmp, w1))\n",
    "print xTr.shape, xTe.shape\n",
    "\n",
    "\n",
    "train = theano.function(inputs=[X], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=x_x, allow_input_downcast=True)\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    iJump = 40\n",
    "\n",
    "    for start, end in zip(range(0, len(xTr), iJump), range(iJump, len(xTr), iJump)):\n",
    "        cost = train(xTr[start:end])\n",
    "        #make sure weights stay tied together\n",
    "        wtVal = 0.5 * (w_h.get_value() + np.transpose(w_o.get_value()))\n",
    "        w_h.set_value(wtVal)\n",
    "        w_o.set_value(np.transpose(wtVal))\n",
    "    dum, outTr = model(xTr[start:end], w_h, w_o, 0.0, 0.0)\n",
    "    dum, outTe = model(xTe, w_h, w_o, 0.0, 0.0)\n",
    "    print i, np.mean(np.square(xTr[start:end] - outTr.eval())), np.mean(np.square(xTe - outTe.eval()))\n",
    "    filename = 'wh2_450x225'\n",
    "    writeFile(w_h.get_value(), filename)\n",
    "\n",
    "    #asList = list(itertools.chain.from_iterable((w_h.get_value()).tolist()))\n",
    "    #plt.hist(asList)\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----in-sample-------out-sample  \n",
    "0 0.010870159608 0.0104834690039  \n",
    "1 0.00825208722435 0.00797470370934  \n",
    "2 0.00710059428044 0.00684058742782  \n",
    "3 0.00642624539801 0.00615923461415  \n",
    "4 0.00597277514473 0.00569340612338  \n",
    "5 0.0056449302224 0.0053501671885  \n",
    "6 0.00539468920043 0.00508671921458  \n",
    "7 0.00519488359327 0.0048773264877  \n",
    "8 0.00503135127701 0.00470778592707  \n",
    "9 0.00489191450737 0.0045675123186  \n",
    ":      :                :  \n",
    ":      :                :  \n",
    "69 0.00332224730792 0.00309935234036  \n",
    "70 0.00331477648566 0.00309371076566  \n",
    "71 0.00330752030716 0.00308835436001  \n",
    "72 0.00330044540032 0.00308304784549  \n",
    "73 0.00329322725735 0.00307783421705  \n",
    "74 0.00328622875749 0.00307279897479  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "#read full data sets and pull out subset of training data to simulate semi-supervised problem\n",
    "xTr, xTe, yTr, yTe = mnist(onehot=True)\n",
    "nPts = 10000\n",
    "xTr = xTr[:nPts, :]\n",
    "yTr = yTr[:nPts, :]\n",
    "\n",
    "rfMnistRaw = RandomForestClassifier(n_estimators=100)\n",
    "rfMnistRaw.fit(xTr, yTr)\n",
    "\n",
    "pred = rfMnistRaw.predict(xTe)\n",
    "\n",
    "pred2 = pred.tolist()\n",
    "yTe2 = yTe.tolist()\n",
    "\n",
    "\n",
    "\n",
    "print pred[0], yTe[0,:]\n",
    "print 'Error Rate on Raw Features =  ', float(sum(sum(pred !=  yTe)))/(2.0*len(xTe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##In-class coding\n",
    "1.  Take out the code tying the weights together and to see how it affects the test error.\n",
    "2.  Add another layer to the autoencoder.\n",
    "\n",
    "##Homework\n",
    "To see how effective this semi-supervised learning is, suppose that you've only got 10k labelled MNIST examples.  Try two approaches to building a classifier.  First train random forest on the 10k examples and see what out-of-sample error you get.  Then train random forest again on 10k examples but use 225 features that you get by running each example through the two-layer trained autoencoder stack that we've just built.  How does the performance compare?\n",
    "\n",
    "What could you do to improve the performance?  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
