{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Lecture 5.2 - Methods for Semi-supervised learning\n",
    "\n",
    "##Readings for the lecture\n",
    "https://en.wikipedia.org/wiki/Semi-supervised_learning - General summary\n",
    "\n",
    "http://www.cs.cmu.edu/~zhuxj/pub/CMU-CALD-02-107.pdf - Label propagation\n",
    "\n",
    "http://papers.nips.cc/paper/2506-learning-with-local-and-global-consistency.pdf - Label Spreading\n",
    "\n",
    "##Self training\n",
    "One method for semi-supervised learning is to train a model on the labeled data and then to apply that model to the unlabeled data in order to generate labels.  Then the newly labeled data is include as part of the training set and a new model is trained.  Many classifiers give probability estimates for each of the possible classes.  That makes it possible to only add the new points where the classifier is the most certain.  That makes the process iterative.  New points are added to the training set which alters the model somewhat and results in new points be added.  The code below implements self-learning using random forest as the basic classifier.  The MNIST data are used for illustration.  Some of the MNIST data samples are chosen to be labeled and some are chosen to be unlabeled.  The code incorporates a probability threshold which is set to 0.75 in the code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 7\n",
      "Error Rate =   0.0636\n",
      "Error Rate =   0.0661   #Added Cases=   2586\n",
      "Error Rate =   0.0651   #Added Cases=   2715\n",
      "Error Rate =   0.066   #Added Cases=   2790\n",
      "Error Rate =   0.0639   #Added Cases=   2858\n",
      "Error Rate =   0.0636   #Added Cases=   2899\n",
      "Error Rate =   0.0622   #Added Cases=   2943\n",
      "Error Rate =   0.0653   #Added Cases=   2973\n",
      "Error Rate =   0.065   #Added Cases=   2999\n",
      "Error Rate =   0.065   #Added Cases=   3025\n",
      "Error Rate =   0.0636   #Added Cases=   3054\n"
     ]
    }
   ],
   "source": [
    "__author__ = 'mike_bowles'\n",
    "\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mnistReader import mnist\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#read full data sets and pull out subset of training data to simulate semi-supervised problem\n",
    "xTr, xTe, yTr, yTe = mnist(onehot=False)\n",
    "nUnLab = 5000\n",
    "nLab = 5000\n",
    "pThresh = 0.75\n",
    "\n",
    "xLab = xTr[:nLab, :]\n",
    "xTot = xTr[:nLab + nUnLab, :]\n",
    "\n",
    "yLab = yTr[:nLab]\n",
    "yTot = yTr[:nUnLab + nLab]\n",
    "\n",
    "rfSemi = RandomForestClassifier(n_estimators=100)\n",
    "rfSemi.fit(xLab, yLab)\n",
    "\n",
    "predTe = rfSemi.predict(xTe)\n",
    "print predTe[0], yTe[0]\n",
    "print 'Error Rate =  ', float(sum(predTe != yTe))/len(xTe)\n",
    "\n",
    "\n",
    "#index manipulations to\n",
    "totPred = rfSemi.predict(xTot)\n",
    "totProb = rfSemi.predict_proba(xTot)\n",
    "\n",
    "#identify any prediction probabilities that are above threshold and not already in the label set\n",
    "#for i in in range(nLab + nUnLab)\n",
    "\n",
    "\n",
    "labGtTh = [i for i in range(len(totPred)) if np.amax(totProb[i, :]) > pThresh]\n",
    "newLab = [i for i in labGtTh if i not in range(nLab)]\n",
    "\n",
    "\n",
    "\n",
    "nIter = 10\n",
    "for i in range(nIter):\n",
    "    yTemp = np.hstack((yLab, yTot[newLab]))\n",
    "    xTemp = np.vstack((xLab, xTot[newLab, :]))\n",
    "\n",
    "    rfSemi.fit(xTemp, yTemp)\n",
    "\n",
    "    #index manipulations to\n",
    "    totPred = rfSemi.predict(xTot)\n",
    "    totProb = rfSemi.predict_proba(xTot)\n",
    "\n",
    "    labGtTh = [i for i in range(len(totPred)) if np.amax(totProb[i, :]) > pThresh]\n",
    "    newLab = [i for i in labGtTh if i not in range(nLab)]\n",
    "\n",
    "\n",
    "\n",
    "    predTe = rfSemi.predict(xTe)\n",
    "    print 'Error Rate =  ', float(sum(predTe != yTe))/len(xTe), '  #Added Cases=  ', len(newLab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the results above, the performance doesn't improve dramatically with the incorporation of the self-trained data, given the parametric choices shown in the code.  \n",
    "\n",
    "##In-class coding exercise\n",
    "Alter some of the parameter values in the code above to learn what elements of the problem help or hurt performance improvement from the value achieved by training on the unlabeled cases only.  You can alter the proportion of labeled to unlabeled points and the probability threshold.  \n",
    "\n",
    "##Label Propagation\n",
    "Label propagation operates by building a proximity matrix between all the pairs of points in the data set.  The label propagation algorithm calculates assigns labels to each point based on the labels for all the points that are close to it.  The paper discuses how the Euclidean distance is used to derive weights that are used to calculate an overall score for each possible class based on the number and distance of points of each class in a neighborhood of the point for which a score is being sought.  This results in labels being assigned to the unlabeled points.  The labels for the labeled points are not changed.  The changing labels assigned to the unlabeled points makes label propagation an iterative algorithm.  \n",
    "\n",
    "The code below uses the LabelPropagation program from sklearn.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4824\n"
     ]
    }
   ],
   "source": [
    "__author__ = 'mike_bowles'\n",
    "\n",
    "from sklearn.semi_supervised import LabelPropagation, LabelSpreading\n",
    "import numpy as np\n",
    "from mnistReader import mnist\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#read full data sets and pull out subset of training data to simulate semi-supervised problem\n",
    "xTr, xTe, yTr, yTe = mnist(onehot=False)\n",
    "nUnLab = 5000\n",
    "nLab = 5000\n",
    "\n",
    "\n",
    "lpModel = LabelPropagation(kernel='rbf', gamma=2.0, n_neighbors=13, alpha=0.99, max_iter=10, tol=0.001)\n",
    "\n",
    "x = xTr[:nLab + nUnLab, :]\n",
    "y = yTr[:nLab + nUnLab]\n",
    "y[nLab:nLab + nUnLab] = -1\n",
    "\n",
    "lpModel.fit(x,y)\n",
    "\n",
    "\n",
    "\n",
    "print float(np.sum(lpModel.predict(xTe) == yTe))/float(len(xTe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results here are a little underwheliming.  The out-of-sample performance with the added unlabeled data is worse than without adding it.  That's not good.  At least it can be measured, so you'll know if you're getting any improvement from trying to incorporate the unlabeled data.  The authors admit that the algorithm doesn't consistently lead to improvement.  \n",
    "\n",
    "##In-class coding exercise\n",
    "Twiddle the parameters in the code example above and discuss how the relative sizes of the labeled and unlabeled sets affect the results.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4405\n"
     ]
    }
   ],
   "source": [
    "__author__ = 'mike_bowles'\n",
    "\n",
    "from sklearn.semi_supervised import LabelPropagation, LabelSpreading\n",
    "import numpy as np\n",
    "from mnistReader import mnist\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#read full data sets and pull out subset of training data to simulate semi-supervised problem\n",
    "xTr, xTe, yTr, yTe = mnist(onehot=False)\n",
    "nUnLab = 2000\n",
    "nLab = 2000\n",
    "\n",
    "\n",
    "\n",
    "lsModel = LabelSpreading(kernel='rbf', gamma=3.0, n_neighbors=25, alpha=0.2, max_iter=30, tol=0.001)\n",
    "x = xTr[:nLab + nUnLab, :]\n",
    "y = yTr[:nLab + nUnLab]\n",
    "y[nLab:nLab + nUnLab] = -1\n",
    "\n",
    "lsModel.fit(x,y)\n",
    "\n",
    "print float(np.sum(lsModel.predict(xTe) == yTe))/float(len(xTe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the results aren't spectacular.  \n",
    "\n",
    "##Homework problem\n",
    "Using the autoencoder method on the MNIST data you were getting slightly worse results using the autoencoder weights and random forest than running random forest on the raw data.  Train a four layer network on a 10k sample of the MNIST data, but instead of using random initialization for the weights, start the weights with the weights that came from autoencoder training.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
