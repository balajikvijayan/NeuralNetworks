{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Lecture 6.2: Training a Factorization Machine\n",
    "##What you'll learn in this session\n",
    "1.  Reduce Rendel's equations to code\n",
    "2.  Use what you've learned to improve performance\n",
    "3.  How to adapt the technique for different types of prediction problems\n",
    "\n",
    "##Order of topics\n",
    "1.  Walk through code and compare to Rendle's paper\n",
    "2.  Develop equations for least squares regression\n",
    "3.  Adding L2 penalty to problem statement\n",
    "4.  Checking code for correctness\n",
    "5.  Initialization\n",
    "6.  Modifying equations for different problem types.  \n",
    "\n",
    "##Pre-reading\n",
    "http://www.ismll.uni-hildesheim.de/pub/pdfs/Rendle2010FM.pdf - #Rendle's original paper\n",
    "\n",
    "##Code Walk-through\n",
    "The code in the code section below encapsulates Rendles equations 1 through 4.  A good place to start is the function \"predict\".  That shows python numpy code for generating predictions given the various matrix quantities in Rendles equation 1.  Equation 1 has four basic quantities - a scalar bias ($w_0$), a vector of linear weights ($w_1$ through $w_n$), factor matrix ($v$) and an attribute vectore ($x_1$ through $x_n$).  In the code the notations for the scalar bias is changed from $w_0$ to b.  Other than that the variable names in the code match those in the paper.  The code develops three scalar quantities whose sum constitutes the prediction - one quantity for each of the expressions in Rendle's equation 1.  The first is just the bias values b.  The second is denoted as $a_2$ and the third as $a_3$.  The quantity a2 is the linear term and is simply the dot product of the weight array w with the array of attributes x.  The quantity a3 is the quadratic term and involves the vectors that the paper denotes as $v_i$ in bold letters.  These vectors form the rows of a matrix denoted as bold V (upper case).  The sum involving the attribute vector is equivalent to a quadratic form involving the attribute vector x and a matrix $M = VV^T$.  The tricky bit is that the sum is only over the upper triangle of M.  It doesn't include the diagonal.  In the math for Lemma 3.1, Rendle shows that the third sum in equation 3, is equivalent to forming the quadratic form $x^TVV^Tx$, subtracting the diagonal contribution and then dividing by two.  You'll see that in the code.  First $VV^T$ is formed and then the diagonal elements are set to zero (instead of substracting them after the multiplication).  Then the quadratic form is evaluated and multiplied by a half.  \n",
    "\n",
    "The function named \"grad\" implements equation 4 from the paper.  The gradients with respect to the bias term and the linear term are straightforward.  The implementation of the gradient with respect to the matrix V follows equation 4 except that it builds the matrix a row at a time whereas the equation in the paper is for single elements from V.  There is also a numerical version of the gradient function in order to check calculations.  The numerical gradient is a time consuming way to do the calculation, but the computation is a very straightforward implementation of the definition of a gradient so it's easy to check visually.\n",
    "\n",
    "##In class exercises\n",
    "1  The code for predict and grad define how predictions are made and the gradient of the predictions, but gradient descent needs the gradient of some performance measure with respect to the parameters (b, w and V in this case).  Suppose that the problem being solved is a regression problem and that the performance measure is mean squared error or a training set or test set.  Call the prediction function $P(b, w, V, x)$.  Write the expression for the performance over a set of examples and the gradient of the performance in terms of the gradient of $P$.  \n",
    "\n",
    "2  How would your answer in the exercise above change if you chose to minimize mean absolute error instead of mean squared error?\n",
    "\n",
    "3  The matrix V introduces a large number of free parameters into the solution.  It is likely that some sort of regularization will be required to avoid overfitting.  L2 is the easiest type of regularization to append to the problem.  To see why, write an expression for the L2 penalty of an array of weights and then take its gradient in symbolic form.  If you wanted to add an L2 penalty, what would you add to the gradient of the performance measure?  \n",
    "\n",
    "4  Run the code below and adjust the parameters to improve the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error     Test Error\n",
      "3.67058181397 3.80384215942\n",
      "2.76747788435 2.87455673336\n",
      "2.27522773623 2.36769409713\n",
      "5.32837041316 5.37935705912\n",
      "2.21039301464 2.3163793714\n",
      "1.95227755383 2.04459715013\n",
      "1.72256840613 1.80796931797\n",
      "1.51848414245 1.59782465876\n",
      "1.34875246644 1.41818285787\n",
      "1.22282712222 1.27821241807\n",
      "final train and test error =  1.1451262263 1.18665923168\n",
      "b =  0.0103010095673\n",
      "w =  [ 0.08570646  0.00489227  0.00262736  0.02231197  0.00081567  0.08138491\n",
      "  0.02534169  0.01026169  0.03439232  0.00678789  0.11211537]\n",
      "v = \n",
      "[[  1.79792795e-02   3.57217874e-02   5.69684141e-02   4.54583888e-02\n",
      "   -3.78942957e-02]\n",
      " [  5.75565269e-03   1.14820449e-02   6.19195338e-03  -1.76075195e-02\n",
      "    2.56073621e-02]\n",
      " [  3.52021622e-02  -5.14779896e-03   9.50928997e-03  -3.91181685e-03\n",
      "   -6.26953144e-03]\n",
      " [ -4.91828663e-03   2.03072014e-02  -2.19503209e-02   4.87997669e-02\n",
      "   -2.48129519e-02]\n",
      " [ -5.23854890e-03   5.80503616e-03  -3.00243792e-02  -1.92368586e-02\n",
      "   -1.03696782e-02]\n",
      " [  3.18649320e-02   1.88806697e-02  -1.08775807e-02  -4.01596508e-03\n",
      "   -4.12074006e-03]\n",
      " [ -2.82012204e-02  -1.35718853e-02   2.53663603e-02   6.87643422e-03\n",
      "   -3.20322134e-05]\n",
      " [ -1.42730288e-02   3.16502045e-02  -8.52620784e-03   1.53765453e-02\n",
      "   -3.32285075e-02]\n",
      " [  9.16434696e-03   2.19017481e-02   5.09388536e-02  -5.32701446e-03\n",
      "    4.66796546e-03]\n",
      " [ -1.36534576e-02   1.61019574e-02   2.38529831e-02   1.63257307e-02\n",
      "    1.96506584e-02]\n",
      " [  2.10499064e-02   3.57500012e-02   6.78475433e-02   3.68813155e-02\n",
      "   -3.40715324e-02]]\n"
     ]
    }
   ],
   "source": [
    "__author__ = 'mike-bowles'\n",
    "import urllib2\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def predict(b, w, v, x): #b=scalar, w=np.array, v=np 2-dim array\n",
    "    #dimensions\n",
    "    #dim(x) = nAtt\n",
    "    #dim(b) = python float\n",
    "    #dim(w) = nAtt\n",
    "    #dim(v) = nAtt x nDim\n",
    "    #function return = nRow x 1\n",
    "    a2 = np.dot(w, x)\n",
    "    M = np.dot(v, np.transpose(v))\n",
    "    i,j = np.indices(M.shape)\n",
    "    M[i==j] = 0.0\n",
    "    a3 = 0.5 * np.dot(x, np.dot(M, x))\n",
    "    return b + a2 + a3\n",
    "\n",
    "def grad(b, w, v, x):\n",
    "    #x is single row from data in array form (not single column matrix\n",
    "    #call of the form grad(,,,X[i,:])\n",
    "    bGrad = 1.0\n",
    "    wGrad = np.array(x)\n",
    "    vGrad = np.zeros_like(v)\n",
    "    vSum = np.zeros_like(v[1, :])\n",
    "    for j in range(len(x)):\n",
    "        vSum += v[j, :] * x[j]\n",
    "    for i in range(len(x)):\n",
    "        vGrad[i, :] = x[i] * vSum - v[i, :] * x[i] * x[i]\n",
    "    return bGrad, wGrad, vGrad\n",
    "\n",
    "def gradNum(b, w, v, x):\n",
    "    eps = max(b * 1e-4, 1e-6)\n",
    "    pr = predict(b, w, v, x)\n",
    "    pbPlus = predict(b + eps, w, v, x)\n",
    "    bGrad = (pbPlus - pr) / eps\n",
    "    wGrad = np.zeros_like(w)\n",
    "    for i in range(len(w)):\n",
    "        eps = max(w[i] * 1e-4, 1e-6)\n",
    "        wPlus = np.array(w)\n",
    "        wPlus[i] = w[i] + eps\n",
    "        pwPlus = predict(b, wPlus, v, x)\n",
    "        wGrad[i] = (pwPlus - pr) / eps\n",
    "    (vi, vj) = v.shape\n",
    "    vGrad = np.zeros_like(v)\n",
    "    for i in range(vi):\n",
    "        for j in range(vj):\n",
    "            eps = max(v[i,j] * 1e-4, 1e-6)\n",
    "            vPlus = np.array(v)\n",
    "            vPlus[i, j] = v[i,j] + eps\n",
    "            pvPlus = predict(b, w, vPlus, x)\n",
    "            vGrad[i,j] = (pvPlus - pr) / eps\n",
    "    return bGrad, wGrad, vGrad\n",
    "\n",
    "#read data\n",
    "target_url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "data = urllib2.urlopen(target_url)\n",
    "x = []\n",
    "y = []\n",
    "names = []\n",
    "firstLine = True\n",
    "for row in data:\n",
    "    if firstLine:\n",
    "        names = row.strip().split(\";\")\n",
    "        firstLine = False\n",
    "    else:\n",
    "        rowSplit = row.strip().split(\";\")\n",
    "        y.append(float(rowSplit.pop()))\n",
    "\n",
    "        floatRow = [float(num) for num in rowSplit]\n",
    "        x.append(floatRow)\n",
    "\n",
    "#training and test sets\n",
    "indices = range(len(x))\n",
    "xTe = [x[i] for i in indices if i%3 == 0 ]\n",
    "xTr = [x[i] for i in indices if i%3 != 0 ]\n",
    "yTe = [y[i] for i in indices if i%3 == 0]\n",
    "yTr = [y[i] for i in indices if i%3 != 0]\n",
    "\n",
    "xTe = np.array(xTe); xTr = np.array(xTr); yTe = np.array(yTe); yTr = np.array(yTr)\n",
    "\n",
    "#meta parameters\n",
    "nDim = 5   #dimension of abstract factor space\n",
    "lam1 = 0.001 #multiplier for L2 penalty on linear weight vector w\n",
    "lam2 = 0.1 #multiplier for L2 penalty on cross-product terms v\n",
    "nPasses = 100 #number of passes through gradient descent calculations\n",
    "gradTest = False  #test gradient calculation against numerical gradient?\n",
    "gradStep = 0.0001  #step size for gradient descent.\n",
    "\n",
    "#some setup and initialization\n",
    "rowTr, nAtt = xTr.shape\n",
    "rowTe, nAtt = xTe.shape\n",
    "vSize = nDim * nAtt\n",
    "wSize = nAtt\n",
    "v = np.random.normal(0.0, 1.0 / float(vSize), (len(xTr[0, :]), nDim))\n",
    "#v = np.zeros((len(xTr[0, :]), nDim))\n",
    "b = 0.0\n",
    "#w = np.random.normal(0.0, 1.0 / float(wSize), (len(xTr[0, :])))\n",
    "w = np.zeros_like(xTr[0, :])\n",
    "\n",
    "print 'Training Error     Test Error'\n",
    "for iPass in range(nPasses):\n",
    "    #calculate full batch gradient (whole training set)\n",
    "    bGrad = 0.0; wGrad = np.zeros_like(xTr[0, :]); vGrad = np.zeros_like(v)\n",
    "    for i in range(rowTr):\n",
    "        x = xTr[i, :]\n",
    "        bg, wg, vg = grad(b, w, v, x)\n",
    "        if gradTest:\n",
    "            bgNum, wgNum, vgNum = gradNum(b, w, v, x)\n",
    "            print bg - bgNum, np.linalg.norm(wg - wgNum), np.linalg.norm(vg - vgNum)\n",
    "        err = yTr[i] - predict(b, w, v, x)\n",
    "        bGrad -= err * bg/float(rowTr)\n",
    "        wGrad -= err * wg/float(rowTr)\n",
    "        vGrad -= err * vg/float(rowTr)\n",
    "    #include regularization penalties and take the step\n",
    "    b -= gradStep * bGrad\n",
    "    w -= gradStep * (wGrad + lam1 * w)\n",
    "    v -= gradStep * (vGrad + lam2 * v)\n",
    "    #calc and print result to monitor progress\n",
    "    eTr = 0.0\n",
    "    eTe = 0.0\n",
    "    for i in range(rowTr):\n",
    "        x = xTr[i, :]\n",
    "        err = (yTr[i] - predict(b, w, v, x))\n",
    "        eTr += err * err / rowTr\n",
    "    for i in range(rowTe):\n",
    "        x = xTe[i, :]\n",
    "        err = (yTe[i] - predict(b, w, v, x))\n",
    "        eTe += err * err / rowTe\n",
    "    if iPass%10 == 0: print sqrt(eTr), sqrt(eTe)\n",
    "\n",
    "print 'final train and test error = ', sqrt(eTr), sqrt(eTe)\n",
    "print 'b = ', b\n",
    "print 'w = ', w\n",
    "print 'v = '\n",
    "print v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##In-class coding\n",
    "1.  Modify the code above to minimize mean absolute error instead of mean squared error. \n",
    "\n",
    "\n",
    "##Homework problems\n",
    "1  Implement backtracking line search to improve gradient descent behavior\n",
    "\n",
    "2  Suppose you're given a classification problem.  Suppose y takes values +/- 1.0 and call the prediction function f(x).  Produce the code that takes the gradients of the prediction function wrt to b, w and v (named bGrad, wGrad and vGrad in the code above) and calculates the loss function gradients for logistic loss ($\\frac{1}{ln\\,2}ln(1\\,+\\,e^{-y\\,f(x)})$)\n",
    "\n",
    "3  With the same suppositions as in 2, produce the code for hinge loss function ($max(0.0, 1\\,-\\,y\\,f(x))$). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
