{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Recurrent Neural Nets\n",
    "\n",
    "##What you'll learn\n",
    "1.  What is a recurrent neural network\n",
    "2.  What kinds of problems are recurrent networks best suited for\n",
    "3.  What are the architectural choices for a recurrent neural net\n",
    "\n",
    "##Order of topics\n",
    "1.  Introduction to recurrent networks\n",
    "2.  Ramalho's Theano code for Elman network\n",
    "3.  Discussion of Theano scan function\n",
    "4.  Stock price prediction with recurrent net\n",
    "5.  Karpathy's 100-line python character-based language model\n",
    "\n",
    "##Reading material\n",
    "https://www.willamette.edu/~gorr/classes/cs449/rnn1.html - class notes from Gorr's RNN lecture\n",
    "\n",
    "http://arxiv.org/pdf/1506.00019v4.pdf - Lipton's excellent review of RNN\n",
    "\n",
    "http://www.nehalemlabs.net/prototype/blog/2013/10/10/implementing-a-recurrent-neural-network-in-python/ - Discussion of RNN and theano code for Elman network - Tiago Ramalho\n",
    "\n",
    "http://deeplearning.net/software/theano/library/scan.html#module-theano.scan_module - Read this documentation on theano scan function and run through some of the example code for its use.  \n",
    "\n",
    "##Intro to RNN\n",
    "Network models and math from various of the readings. \n",
    "\n",
    "##Theano Scan function\n",
    "You've seen from the intro that with RNN's, the value of the hidden layer output is a function of the input and of the past hidden layer outputs.  The theano scan function stores as many past values of the hidden \n",
    "\n",
    "http://deeplearning.net/software/theano/library/scan.html#lib-scan - check examples and scroll 2/3 down page to \"reference\" for details.\n",
    "\n",
    "http://deeplearning.net/software/theano/tutorial/loop.html\n",
    "\n",
    "Here are a couple of the examples from the documentation on the scan function.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   1.   4.   9.  16.  25.  36.  49.  64.  81.]\n",
      "[  0.00000000e+00   1.00000000e+00   1.60000000e+01   8.10000000e+01\n",
      "   2.56000000e+02   6.25000000e+02   1.29600000e+03   2.40100000e+03\n",
      "   4.09600000e+03   6.56100000e+03]\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "k = T.iscalar(\"k\")\n",
    "A = T.vector(\"A\")\n",
    "\n",
    "# Symbolic description of the result\n",
    "result, updates = theano.scan(fn=lambda prior_result, A: prior_result * A,\n",
    "                              outputs_info=T.ones_like(A),\n",
    "                              non_sequences=A,\n",
    "                              n_steps=k)\n",
    "\n",
    "# We only care about A**k, but scan has provided us with A**1 through A**k.\n",
    "# Discard the values that we don't care about. Scan is smart enough to\n",
    "# notice this and not waste memory saving them.\n",
    "final_result = result[-1]\n",
    "\n",
    "# compiled function that returns A**k\n",
    "power = theano.function(inputs=[A,k], outputs=final_result, updates=updates)\n",
    "\n",
    "print power(range(10),2)\n",
    "print power(range(10),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Q's\n",
    "1  Match lambda function inputs to the other arguments in the scan arg list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.0\n",
      "19.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(12.0, dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "coefficients = theano.tensor.vector(\"coefficients\")\n",
    "x = T.scalar(\"x\")\n",
    "\n",
    "max_coefficients_supported = 10000\n",
    "\n",
    "# Generate the components of the polynomial\n",
    "components, updates = theano.scan(fn=lambda coefficient, power, free_variable: coefficient * (free_variable ** power),\n",
    "                                  outputs_info=None,\n",
    "                                  sequences=[coefficients, theano.tensor.arange(max_coefficients_supported)],\n",
    "                                  non_sequences=x)\n",
    "# Sum them up\n",
    "polynomial = components.sum()\n",
    "\n",
    "# Compile a function\n",
    "calculate_polynomial = theano.function(inputs=[coefficients, x], outputs=polynomial)\n",
    "\n",
    "# Test\n",
    "test_coefficients = numpy.asarray([1, 0, 2], dtype=numpy.float32)\n",
    "test_value = 3\n",
    "print calculate_polynomial(test_coefficients, test_value)\n",
    "print 1.0 * (3 ** 0) + 0.0 * (3 ** 1) + 2.0 * (3 ** 2)\n",
    "g = T.grad(polynomial, x)\n",
    "calc_grad = theano.function([coefficients, x], g)\n",
    "calc_grad(test_coefficients, test_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Q\n",
    "1.  Write an expression for the derivative of the polynomial that scan is calculating in the code above.  Evaluate it at a few values of x and see if the answers compare.\n",
    "2.  Rerun the experiment in question 1 (immediately above) for some different polynomials. \n",
    "3.  Explain how the gradient is being calculated through the scan operation.  What are the steps that scan is taking?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def oneStep(u_tm4, u_t, x_tm3, x_tm1, y_tm1, W, W_in_1, W_in_2,  W_feedback, W_out):\n",
    "\n",
    "  x_t = T.tanh(theano.dot(x_tm1, W) + \\\n",
    "               theano.dot(u_t,   W_in_1) + \\\n",
    "               theano.dot(u_tm4, W_in_2) + \\\n",
    "               theano.dot(y_tm1, W_feedback))\n",
    "  y_t = theano.dot(x_tm3, W_out)\n",
    "\n",
    "  return [x_t, y_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = T.matrix()\n",
    "W_in_1 = T.matrix()\n",
    "W_in_2 = T.matrix()\n",
    "W_feedback = T.matrix()\n",
    "W_out = T.matrix()\n",
    "\n",
    "u = T.matrix() # it is a sequence of vectors\n",
    "x0 = T.matrix() # initial state of x has to be a matrix, since\n",
    "                # it has to cover x[-3]\n",
    "y0 = T.vector() # y0 is just a vector since scan has only to provide\n",
    "                # y[-1]\n",
    "\n",
    "\n",
    "([x_vals, y_vals], updates) = theano.scan(fn=oneStep,\n",
    "                                          sequences=dict(input=u, taps=[-4,-0]),\n",
    "                                          outputs_info=[dict(initial=x0, taps=[-3,-1]), y0],\n",
    "                                          non_sequences=[W, W_in_1, W_in_2, W_feedback, W_out],\n",
    "                                          strict=True)\n",
    "     # for second input y, scan adds -1 in output_taps by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##A Theano RNN\n",
    "The code below implements a single layer Elman network in theano.  The code was written by Tiago Ramalho and comes from his github page - https://gist.github.com/tmramalho/5e8fda10f99233b2370f.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: 2.09426307678\n",
      "iteration 10000: 0.0625283718109\n",
      "iteration 20000: 0.719514608383\n",
      "iteration 30000: 0.995535969734\n",
      "iteration 40000: 0.681394934654\n",
      "iteration 50000: 0.376378536224\n",
      "iteration 60000: 0.0633955001831\n",
      "iteration 70000: 0.23927283287\n",
      "iteration 80000: 0.119526267052\n",
      "iteration 90000: 0.564009547234\n",
      "iteration 100000: 0.351581573486\n",
      "iteration 110000: 0.217575073242\n",
      "iteration 120000: 0.16767680645\n",
      "iteration 130000: 0.0334808826447\n",
      "iteration 140000: 0.0208132266998\n",
      "iteration 150000: 0.230397224426\n",
      "iteration 160000: 0.113218545914\n",
      "iteration 170000: 0.0179162025452\n",
      "iteration 180000: 0.196667671204\n",
      "iteration 190000: 0.056088924408\n",
      "iteration 200000: 0.141449928284\n",
      "iteration 210000: 0.00644361972809\n",
      "iteration 220000: 0.0299093723297\n",
      "iteration 230000: 0.00022554397583\n",
      "iteration 240000: 0.0248272418976\n",
      "iteration 250000: 0.0303966999054\n",
      "iteration 260000: 0.0143444538116\n",
      "iteration 270000: 0.0223622322083\n",
      "iteration 280000: 0.0114002227783\n",
      "iteration 290000: 0.00817358493805\n",
      "iteration 300000: 0.0429527759552\n",
      "iteration 310000: 0.00111746788025\n",
      "iteration 320000: 0.00869750976562\n",
      "iteration 330000: 0.0028223991394\n",
      "iteration 340000: 0.0176405906677\n",
      "iteration 350000: 0.0535662174225\n",
      "iteration 360000: 0.028787612915\n",
      "iteration 370000: 0.0236277580261\n",
      "iteration 380000: 0.0168440341949\n",
      "iteration 390000: 0.0048189163208\n",
      "iteration 400000: 0.0287661552429\n",
      "iteration 410000: 0.0636155605316\n",
      "iteration 420000: 0.0591952800751\n",
      "iteration 430000: 0.0233573913574\n",
      "iteration 440000: 0.00617909431458\n",
      "iteration 450000: 0.00417256355286\n",
      "iteration 460000: 0.0122680664062\n",
      "iteration 470000: 0.0297477245331\n",
      "iteration 480000: 0.00351643562317\n",
      "iteration 490000: 0.00906801223755\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEACAYAAAC57G0KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n12sOoLUFncsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d56c39b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from https://gist.github.com/tmramalho/5e8fda10f99233b2370f\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import cPickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class RNN(object):\n",
    "\n",
    "    def __init__(self, nin, n_hidden, nout):\n",
    "        rng = np.random.RandomState(1234)\n",
    "        W_uh = np.asarray(rng.normal(size=(nin, n_hidden), scale= .01, loc = .0), dtype = theano.config.floatX)\n",
    "        W_hh = np.asarray(rng.normal(size=(n_hidden, n_hidden), scale=.01, loc = .0), dtype = theano.config.floatX)\n",
    "        W_hy = np.asarray(rng.normal(size=(n_hidden, nout), scale =.01, loc=0.0), dtype = theano.config.floatX)\n",
    "        b_hh = np.zeros((n_hidden,), dtype=theano.config.floatX)\n",
    "        b_hy = np.zeros((nout,), dtype=theano.config.floatX)\n",
    "        self.activ = T.nnet.sigmoid\n",
    "        lr = T.scalar()\n",
    "        u = T.matrix()\n",
    "        t = T.scalar()\n",
    "\n",
    "        W_uh = theano.shared(W_uh, 'W_uh')\n",
    "        W_hh = theano.shared(W_hh, 'W_hh')\n",
    "        W_hy = theano.shared(W_hy, 'W_hy')\n",
    "        b_hh = theano.shared(b_hh, 'b_hh')\n",
    "        b_hy = theano.shared(b_hy, 'b_hy')\n",
    "\n",
    "        h0_tm1 = theano.shared(np.zeros(n_hidden, dtype=theano.config.floatX))\n",
    "\n",
    "        h, _ = theano.scan(self.recurrent_fn, sequences = u,\n",
    "                           outputs_info = [h0_tm1],\n",
    "                           non_sequences = [W_hh, W_uh, W_hy, b_hh])\n",
    "\n",
    "        y = T.dot(h[-1], W_hy) + b_hy\n",
    "        cost = ((t - y)**2).mean(axis=0).sum()\n",
    "\n",
    "        gW_hh, gW_uh, gW_hy,\\\n",
    "            gb_hh, gb_hy = T.grad(\n",
    "            cost, [W_hh, W_uh, W_hy, b_hh, b_hy])\n",
    "\n",
    "        self.train_step = theano.function([u, t, lr], cost,\n",
    "            on_unused_input='warn',\n",
    "            updates=[(W_hh, W_hh - lr*gW_hh),\n",
    "            (W_uh, W_uh - lr*gW_uh),\n",
    "            (W_hy, W_hy - lr*gW_hy),\n",
    "            (b_hh, b_hh - lr*gb_hh),\n",
    "            (b_hy, b_hy - lr*gb_hy)],\n",
    "            allow_input_downcast=True)\n",
    "\n",
    "    def recurrent_fn(self, u_t, h_tm1, W_hh, W_uh, W_hy, b_hh):\n",
    "        h_t = self.activ(T.dot(h_tm1, W_hh) + T.dot(u_t, W_uh) + b_hh)\n",
    "        return h_t\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    rnn = RNN(2, 20, 1)\n",
    "    lr = 0.01\n",
    "    e = 1\n",
    "    vals = []\n",
    "    for i in xrange(int(5e5)):\n",
    "        u = np.random.rand(10,2)\n",
    "        t = np.dot(u[:,0], u[:,1])\n",
    "        c = rnn.train_step(u, t, lr)\n",
    "        if i%10000 == 0: print \"iteration {0}: {1}\".format(i, np.sqrt(c))\n",
    "        e = 0.1*np.sqrt(c) + 0.9*e\n",
    "        if i % 1000 == 0:\n",
    "            vals.append(e)\n",
    "    plt.plot(vals)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##Q\n",
    "1.  How frequently is the gradient being calculated in the code above?\n",
    "2.  How frequently is a gradient step being taken?\n",
    "\n",
    "##Stock Price Prediction\n",
    "The code below adapts Ramalho's rnn code to the task of predicting stock prices.  The input data for the rnn is generated by a short python program (framer.py) that reads the raw data and generates attribute and label files.  There's a file folder containing daily data for all the stocks in the S&P 500.  The dataset includes about 3000 days of data - a little more than 10 years for each of the 500 stocks included in the index.  For each day, there are 5 numeric values - four prices (open high low close) and one volume.  The python program that generates the input has a couple of parameters - how many days of history to include in the attributes and how many days into the future to make the prediction.  The data being used in the rnn below uses one day of past data and generates a prediction of the change in closing price days into the future.  The program framer.py indicates that the label standard variation is about 3.5 for the symbol \"a\" that is the data source for the program below.  Compare that to the smoothed rms error in predicting the 5 day change.  What r-squared would you say the rnn is generating?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: 1.28242719173\n",
      "iteration 10: 2.1814520359\n",
      "iteration 20: 10.0960464478\n",
      "iteration 30: 3.39931869507\n",
      "iteration 40: 2.53961920738\n",
      "iteration 50: 5.02067708969\n",
      "iteration 60: 12.6134777069\n",
      "iteration 70: 21.8684120178\n",
      "iteration 80: 7.81989955902\n",
      "iteration 90: 13.2518558502\n",
      "iteration 100: 10.9278945923\n",
      "iteration 110: 1.4697728157\n",
      "iteration 120: 19.3817806244\n",
      "iteration 130: 4.69277095795\n",
      "iteration 140: 4.93961524963\n",
      "iteration 150: 3.66370368004\n",
      "iteration 160: 6.11796569824\n",
      "iteration 170: 9.18711090088\n",
      "iteration 180: 4.71939945221\n",
      "iteration 190: 3.8073246479\n",
      "iteration 200: 2.89441585541\n",
      "iteration 210: 0.753311157227\n",
      "iteration 220: 12.8787851334\n",
      "iteration 230: 1.00186002254\n",
      "iteration 240: 3.18443202972\n",
      "iteration 250: 1.61186552048\n",
      "iteration 260: 0.0607705116272\n",
      "iteration 270: 6.45117044449\n",
      "iteration 280: 1.45480668545\n",
      "iteration 290: 1.54165410995\n",
      "iteration 300: 0.215069770813\n",
      "iteration 310: 4.22497177124\n",
      "iteration 320: 6.48544168472\n",
      "iteration 330: 1.43369174004\n",
      "iteration 340: 0.386434793472\n",
      "iteration 350: 3.25359249115\n",
      "iteration 360: 0.668283760548\n",
      "iteration 370: 0.33740156889\n",
      "iteration 380: 0.142730236053\n",
      "iteration 390: 2.70097017288\n",
      "iteration 400: 4.43018722534\n",
      "iteration 410: 0.484650611877\n",
      "iteration 420: 2.05805206299\n",
      "iteration 430: 0.961124837399\n",
      "iteration 440: 1.81656336784\n",
      "iteration 450: 1.64063429832\n",
      "iteration 460: 0.339570760727\n",
      "iteration 470: 0.291549921036\n",
      "iteration 480: 1.38466131687\n",
      "iteration 490: 1.00876629353\n",
      "iteration 500: 2.04536628723\n",
      "iteration 510: 0.987689971924\n",
      "iteration 520: 0.0471896529198\n",
      "iteration 530: 1.8059015274\n",
      "iteration 540: 1.11430358887\n",
      "iteration 550: 1.69042062759\n",
      "iteration 560: 0.726953685284\n",
      "iteration 570: 2.12703275681\n",
      "iteration 580: 3.05482506752\n",
      "iteration 590: 2.16900634766\n",
      "iteration 600: 2.4018933773\n",
      "iteration 610: 0.316648125648\n",
      "iteration 620: 3.02155542374\n",
      "iteration 630: 0.0776810646057\n",
      "iteration 640: 0.0821902751923\n",
      "iteration 650: 0.536505103111\n",
      "iteration 660: 2.04649686813\n",
      "iteration 670: 2.54822587967\n",
      "iteration 680: 0.545087039471\n",
      "iteration 690: 2.01806712151\n",
      "iteration 700: 0.030303299427\n",
      "iteration 710: 0.206246733665\n",
      "iteration 720: 0.268271744251\n",
      "iteration 730: 0.159025251865\n",
      "iteration 740: 1.56613087654\n",
      "iteration 750: 2.36541843414\n",
      "iteration 760: 1.99589419365\n",
      "iteration 770: 0.791562616825\n",
      "iteration 780: 0.178558886051\n",
      "iteration 790: 0.40519797802\n",
      "iteration 800: 2.91510748863\n",
      "iteration 810: 1.4965441227\n",
      "iteration 820: 0.0839233621955\n",
      "iteration 830: 0.179445505142\n",
      "iteration 840: 0.529053509235\n",
      "iteration 850: 0.92569231987\n",
      "iteration 860: 0.498330354691\n",
      "iteration 870: 1.03388369083\n",
      "iteration 880: 0.493113398552\n",
      "iteration 890: 0.29975605011\n",
      "iteration 900: 0.860032081604\n",
      "iteration 910: 0.836432218552\n",
      "iteration 920: 0.423968940973\n",
      "iteration 930: 1.1368817091\n",
      "iteration 940: 1.21793353558\n",
      "iteration 950: 0.824265658855\n",
      "iteration 960: 1.15753793716\n",
      "iteration 970: 1.27641642094\n",
      "iteration 980: 1.31437039375\n",
      "iteration 990: 0.057240486145\n",
      "iteration 1000: 0.17281037569\n",
      "iteration 1010: 1.68248200417\n",
      "iteration 1020: 0.562921404839\n",
      "iteration 1030: 0.940418124199\n",
      "iteration 1040: 0.626754760742\n",
      "iteration 1050: 2.1146068573\n",
      "iteration 1060: 1.43734359741\n",
      "iteration 1070: 0.254319876432\n",
      "iteration 1080: 2.05936741829\n",
      "iteration 1090: 0.120587348938\n",
      "iteration 1100: 0.883896946907\n",
      "iteration 1110: 1.23404407501\n",
      "iteration 1120: 0.184114396572\n",
      "iteration 1130: 1.55077135563\n",
      "iteration 1140: 1.02832090855\n",
      "iteration 1150: 1.59009504318\n",
      "iteration 1160: 0.280957102776\n",
      "iteration 1170: 0.0465146303177\n",
      "iteration 1180: 0.190230309963\n",
      "iteration 1190: 0.224103555083\n",
      "iteration 1200: 1.01154971123\n",
      "iteration 1210: 0.444595396519\n",
      "iteration 1220: 0.309847354889\n",
      "iteration 1230: 1.76775598526\n",
      "iteration 1240: 0.44158488512\n",
      "iteration 1250: 0.868898510933\n",
      "iteration 1260: 0.825538516045\n",
      "iteration 1270: 0.0126251876354\n",
      "iteration 1280: 0.0747006461024\n",
      "iteration 1290: 0.687615990639\n",
      "iteration 1300: 0.704619884491\n",
      "iteration 1310: 0.167435467243\n",
      "iteration 1320: 1.03350329399\n",
      "iteration 1330: 0.937483549118\n",
      "iteration 1340: 0.0239095389843\n",
      "iteration 1350: 0.375268369913\n",
      "iteration 1360: 0.935703396797\n",
      "iteration 1370: 0.250578641891\n",
      "iteration 1380: 0.338355123997\n",
      "iteration 1390: 0.0186008512974\n",
      "iteration 1400: 0.76952701807\n",
      "iteration 1410: 1.94693470001\n",
      "iteration 1420: 0.403578460217\n",
      "iteration 1430: 1.23944652081\n",
      "iteration 1440: 2.90176916122\n",
      "iteration 1450: 0.0691585540771\n",
      "iteration 1460: 0.197951853275\n",
      "iteration 1470: 0.540798783302\n",
      "iteration 1480: 0.50632417202\n",
      "iteration 1490: 0.712444961071\n",
      "iteration 1500: 0.98957324028\n",
      "iteration 1510: 0.77824139595\n",
      "iteration 1520: 0.916199922562\n",
      "iteration 1530: 0.788925290108\n",
      "iteration 1540: 1.06841421127\n",
      "iteration 1550: 0.242389410734\n",
      "iteration 1560: 0.757370710373\n",
      "iteration 1570: 0.0413319468498\n",
      "iteration 1580: 0.132726550102\n",
      "iteration 1590: 0.539810061455\n",
      "iteration 1600: 1.22963392735\n",
      "iteration 1610: 0.907009363174\n",
      "iteration 1620: 0.48447316885\n",
      "iteration 1630: 1.3640396595\n",
      "iteration 1640: 1.23211526871\n",
      "iteration 1650: 0.881095647812\n",
      "iteration 1660: 0.568367660046\n",
      "iteration 1670: 0.024775326252\n",
      "iteration 1680: 0.332917094231\n",
      "iteration 1690: 2.04535484314\n",
      "iteration 1700: 0.370792388916\n",
      "iteration 1710: 0.0230146422982\n",
      "iteration 1720: 1.0862724781\n",
      "iteration 1730: 0.127112627029\n",
      "iteration 1740: 0.279580235481\n",
      "iteration 1750: 0.372489988804\n",
      "iteration 1760: 0.838121116161\n",
      "iteration 1770: 0.0494503974915\n",
      "iteration 1780: 0.375923752785\n",
      "iteration 1790: 0.056789547205\n",
      "iteration 1800: 0.0342512726784\n",
      "iteration 1810: 0.508564710617\n",
      "iteration 1820: 0.505513429642\n",
      "iteration 1830: 0.834860026836\n",
      "iteration 1840: 0.732881247997\n",
      "iteration 1850: 0.959815561771\n",
      "iteration 1860: 0.681596934795\n",
      "iteration 1870: 0.350463271141\n",
      "iteration 1880: 0.349639773369\n",
      "iteration 1890: 1.02215850353\n",
      "iteration 1900: 0.448350518942\n",
      "iteration 1910: 0.716925680637\n",
      "iteration 1920: 0.620346844196\n",
      "iteration 1930: 1.01284658909\n",
      "iteration 1940: 0.646267652512\n",
      "iteration 1950: 0.477739214897\n",
      "iteration 1960: 0.137973934412\n",
      "iteration 1970: 0.430220097303\n",
      "iteration 1980: 0.103895813227\n",
      "iteration 1990: 0.422153174877\n",
      "iteration 2000: 0.969086527824\n",
      "iteration 2010: 0.292059004307\n",
      "iteration 2020: 1.37923526764\n",
      "iteration 2030: 2.41921901703\n",
      "iteration 2040: 2.05135536194\n",
      "iteration 2050: 0.348286628723\n",
      "iteration 2060: 2.04274988174\n",
      "iteration 2070: 1.59955370426\n",
      "iteration 2080: 0.384524941444\n",
      "iteration 2090: 0.536097764969\n",
      "iteration 2100: 0.682509839535\n",
      "iteration 2110: 1.16348278522\n",
      "iteration 2120: 0.886954426765\n",
      "iteration 2130: 1.08894181252\n",
      "iteration 2140: 1.12121081352\n",
      "iteration 2150: 0.848816335201\n",
      "iteration 2160: 0.483648538589\n",
      "iteration 2170: 0.00827783346176\n",
      "iteration 2180: 0.218861281872\n",
      "iteration 2190: 0.564344167709\n",
      "iteration 2200: 1.54128098488\n",
      "iteration 2210: 0.715189695358\n",
      "iteration 2220: 1.62990427017\n",
      "iteration 2230: 2.25084590912\n",
      "iteration 2240: 0.965302705765\n",
      "iteration 2250: 1.2434566021\n",
      "iteration 2260: 1.09571051598\n",
      "iteration 2270: 0.747665584087\n",
      "iteration 2280: 0.341537714005\n",
      "iteration 2290: 2.40842056274\n",
      "iteration 2300: 0.235644936562\n",
      "iteration 2310: 1.11074388027\n",
      "iteration 2320: 2.05902457237\n",
      "iteration 2330: 0.438732147217\n",
      "iteration 2340: 0.299773573875\n",
      "iteration 2350: 0.00696988403797\n",
      "iteration 2360: 0.646561741829\n",
      "iteration 2370: 1.36524343491\n",
      "iteration 2380: 0.550951242447\n",
      "iteration 2390: 1.31923472881\n",
      "iteration 2400: 1.0908151865\n",
      "iteration 2410: 0.89830237627\n",
      "iteration 2420: 2.04162812233\n",
      "iteration 2430: 0.281818628311\n",
      "iteration 2440: 0.519325494766\n",
      "iteration 2450: 0.59021371603\n",
      "iteration 2460: 2.15233206749\n",
      "iteration 2470: 1.14160990715\n",
      "iteration 2480: 1.48388886452\n",
      "iteration 2490: 0.467240333557\n",
      "iteration 2500: 1.18262434006\n",
      "iteration 2510: 0.224033117294\n",
      "iteration 2520: 0.75885361433\n",
      "iteration 2530: 0.315900146961\n",
      "iteration 2540: 0.145083904266\n",
      "iteration 2550: 0.268759071827\n",
      "iteration 2560: 1.04735171795\n",
      "iteration 2570: 1.06861376762\n",
      "iteration 2580: 0.242105305195\n",
      "iteration 2590: 0.534836709499\n",
      "iteration 2600: 0.611706733704\n",
      "iteration 2610: 0.813089966774\n",
      "iteration 2620: 1.1917206049\n",
      "iteration 2630: 2.72157001495\n",
      "iteration 2640: 1.80202770233\n",
      "iteration 2650: 2.4268784523\n",
      "iteration 2660: 1.3583111763\n",
      "iteration 2670: 1.62357211113\n",
      "iteration 2680: 0.737380504608\n",
      "iteration 2690: 1.34707915783\n",
      "iteration 2700: 2.36176109314\n",
      "iteration 2710: 1.0845297575\n",
      "iteration 2720: 0.0941797494888\n",
      "iteration 2730: 1.16126656532\n",
      "iteration 2740: 0.406536996365\n",
      "iteration 2750: 0.322044372559\n",
      "iteration 2760: 0.130097463727\n",
      "iteration 2770: 0.446222215891\n",
      "iteration 2780: 0.528184652328\n",
      "iteration 2790: 0.80004799366\n",
      "iteration 2800: 0.727386832237\n",
      "iteration 2810: 0.561152696609\n",
      "iteration 2820: 1.28150033951\n",
      "iteration 2830: 0.360154271126\n",
      "iteration 2840: 3.23287153244\n",
      "iteration 2850: 1.14520597458\n",
      "iteration 2860: 0.642797827721\n",
      "iteration 2870: 1.38821434975\n",
      "iteration 2880: 0.417080402374\n",
      "iteration 2890: 1.73063707352\n",
      "iteration 2900: 1.81989181042\n",
      "iteration 2910: 1.87725090981\n",
      "iteration 2920: 2.68798875809\n",
      "iteration 2930: 1.53005194664\n",
      "iteration 2940: 1.32877445221\n",
      "iteration 2950: 4.79540491104\n",
      "iteration 2960: 2.75889730453\n",
      "iteration 2970: 0.430217146873\n",
      "iteration 2980: 2.02209281921\n",
      "iteration 2990: 1.20771145821\n",
      "iteration 3000: 4.06972360611\n",
      "iteration 3010: 0.404149442911\n",
      "iteration 3020: 1.23351264\n",
      "iteration 3030: 2.6428091526\n",
      "iteration 3040: 1.30823922157\n",
      "iteration 3050: 0.188104391098\n",
      "iteration 3060: 0.195424318314\n",
      "iteration 3070: 2.5627989769\n",
      "iteration 3080: 0.970802783966\n",
      "iteration 3090: 0.694916009903\n",
      "iteration 3100: 0.0465535521507\n",
      "iteration 3110: 2.12422370911\n",
      "iteration 3120: 2.10342764854\n",
      "iteration 3130: 2.47720909119\n",
      "iteration 3140: 0.204406440258\n",
      "iteration 3150: 0.161298513412\n",
      "iteration 3160: 1.07116365433\n",
      "iteration 3170: 1.06470799446\n",
      "iteration 3180: 2.39241838455\n",
      "iteration 3190: 0.34721082449\n",
      "iteration 3200: 4.58884429932\n",
      "iteration 3210: 0.0219486951828\n",
      "iteration 3220: 0.672662258148\n",
      "iteration 3230: 0.0264721214771\n",
      "iteration 3240: 0.885486245155\n",
      "iteration 3250: 1.35787761211\n",
      "iteration 3260: 2.16805148125\n",
      "iteration 3270: 1.59723627567\n",
      "iteration 3280: 1.13122200966\n",
      "iteration 3290: 0.545246601105\n",
      "iteration 3300: 0.832512915134\n",
      "iteration 3310: 0.0618691444397\n",
      "iteration 3320: 0.178183659911\n",
      "iteration 3330: 0.265615344048\n",
      "iteration 3340: 0.787811219692\n",
      "iteration 3350: 0.664319038391\n",
      "iteration 3360: 1.68563294411\n",
      "iteration 3370: 1.18847465515\n",
      "iteration 3380: 1.61371994019\n",
      "iteration 3390: 1.68885242939\n",
      "iteration 3400: 0.632167577744\n",
      "iteration 3410: 1.64404392242\n",
      "iteration 3420: 0.99450802803\n",
      "iteration 3430: 0.464384317398\n",
      "iteration 3440: 1.50314116478\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEACAYAAAC57G0KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmJ47tatBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d7c1ceb10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "__author__ = 'mike.bowles'\n",
    "#based on  code from https://gist.github.com/tmramalho/5e8fda10f99233b2370f\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class RNN(object):\n",
    "\n",
    "    def __init__(self, nin, n_hidden, nout):\n",
    "        rng = np.random.RandomState(1234)\n",
    "        W_uh = np.asarray(rng.normal(size=(nin, n_hidden), scale= .01, loc = 0.0), dtype = theano.config.floatX)\n",
    "        W_hh = np.asarray(rng.normal(size=(n_hidden, n_hidden), scale=.01, loc = 0.0), dtype = theano.config.floatX)\n",
    "        W_hy = np.asarray(rng.normal(size=(n_hidden, nout), scale =.01, loc=0.0), dtype = theano.config.floatX)\n",
    "        b_hh = np.zeros((n_hidden,), dtype=theano.config.floatX)\n",
    "        b_hy = np.zeros((nout,), dtype=theano.config.floatX)\n",
    "        self.activ = T.nnet.sigmoid\n",
    "        lr = T.scalar()\n",
    "        u = T.matrix()\n",
    "        t = T.scalar()\n",
    "\n",
    "        W_uh = theano.shared(W_uh, 'W_uh')\n",
    "        W_hh = theano.shared(W_hh, 'W_hh')\n",
    "        W_hy = theano.shared(W_hy, 'W_hy')\n",
    "        b_hh = theano.shared(b_hh, 'b_hh')\n",
    "        b_hy = theano.shared(b_hy, 'b_hy')\n",
    "\n",
    "        h0_tm1 = theano.shared(np.zeros(n_hidden, dtype=theano.config.floatX))\n",
    "        #theano.printing.debugprint([h0_tm1, u, W_hh, W_uh, W_hy, b_hh, b_hy], print_type=True)\n",
    "        h, _ = theano.scan(self.recurrent_fn, sequences = u,\n",
    "                           outputs_info = [h0_tm1],\n",
    "                           non_sequences = [W_hh, W_uh, W_hy, b_hh])\n",
    "\n",
    "        y = T.dot(h[-1], W_hy) + b_hy\n",
    "        cost = ((t - y)**2).mean(axis=0).sum()\n",
    "\n",
    "        gW_hh, gW_uh, gW_hy,\\\n",
    "            gb_hh, gb_hy = T.grad(\n",
    "            cost, [W_hh, W_uh, W_hy, b_hh, b_hy])\n",
    "        #theano.printing.debugprint([h0_tm1], print_type=True)\n",
    "        self.train_step = theano.function([u, t, lr], cost,\n",
    "            on_unused_input='warn',\n",
    "            updates=[(W_hh, W_hh - lr*gW_hh),\n",
    "            (W_uh, W_uh - lr*gW_uh),\n",
    "            (W_hy, W_hy - lr*gW_hy),\n",
    "            (b_hh, b_hh - lr*gb_hh),\n",
    "            (b_hy, b_hy - lr*gb_hy)],\n",
    "            allow_input_downcast=True)\n",
    "\n",
    "    def recurrent_fn(self, u_t, h_tm1, W_hh, W_uh, W_hy, b_hh):\n",
    "        h_t = self.activ(T.dot(h_tm1, W_hh) + T.dot(u_t, W_uh) + b_hh)\n",
    "        return h_t\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    (xlist, ylist) = pickle.load(open('stockTT.bin', 'rb'))\n",
    "    nInputs = len(xlist[0])\n",
    "    x = np.array(xlist, dtype = theano.config.floatX)\n",
    "    y = np.array(ylist, dtype = theano.config.floatX)\n",
    "    nHidden = 20\n",
    "    nOutputs = 1\n",
    "    rnn = RNN(nInputs, nHidden, nOutputs)\n",
    "    lr = 0.01\n",
    "    e = 1.0\n",
    "    nPasses = 1\n",
    "    vals = []\n",
    "    for i in range(nPasses):\n",
    "        for j in range(len(x)):\n",
    "            u = np.asarray(xlist[j], dtype = theano.config.floatX).reshape((1,nInputs))\n",
    "            t = y[j]\n",
    "\n",
    "            c = rnn.train_step(u, t, lr)\n",
    "            if j%10==0: print \"iteration {0}: {1}\".format(j, np.sqrt(c))\n",
    "            e = 0.1*np.sqrt(c) + 0.9*e\n",
    "            vals.append(e)\n",
    "    plt.plot(vals)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Q\n",
    "1.  The network above takes in one day's worth of data and predicts the change in price five days into the future.  How is it making a prediction based on only one day?\n",
    "2.  What things might you change to improve the performance of the network?\n",
    "\n",
    "##Homework\n",
    "Try changing the following things in the stock price prediction network and see what effect your changes have on the performance of the network.  \n",
    "1.  More than one day's of history for input to the network.\n",
    "2.  Wider hidden layer.\n",
    "3.  Truncate the BPTT gradient calculation.\n",
    "4.  Add another hidden layer to the rnn.\n",
    "5.  Alter the activation function.  (e.g. try elu). \n",
    "6.  Try a different gradient algo\n",
    "7.  Try absolute value of error instead of squared error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 30088 characters, 58 unique.\n",
      "----\n",
      " :cMM.pcBEPrK:B Njg\n",
      "lYi.cYenOapM\n",
      "?OBN;CqG EnEdUs?dlV:BhS.vMLHPTIzIKOyK-l;Y!YWxPzAeqSWCIvC w-B';'iFBugTGHE;ghoDrkTITBpDKmHV:nWoSS zIKDHVts:Rqqqzp.ILFvWG!FGWpOsIKB:hl:Tx!C?hs;NkWB,..KdNNFrB:TtIV!qyrMe?dj \n",
      "----\n",
      "iter 0, loss: 101.511079\n",
      "----\n",
      " yvss woefais\n",
      "o nseseiitoile d nl tlivv lhldi,,etFteed itrldenlr wyv ,e  liiirml wem fvvde,tyonsgkeesynenyftonl lsmwutitof\n",
      " l i ii goedeton \n",
      " sridlnetobstolo soef si lo wivsn;eeyice gessdwdyy\n",
      "byoslct,s \n",
      "----\n",
      "iter 100, loss: 101.464662\n",
      "----\n",
      " thetl?u  h thotythat th hh thet, tht tht . th oo whshthtd, ,ati shouotheT \n",
      "h thoote tod\n",
      "Dh cenwho nh ththlth t\n",
      "n thsitol thyht?t itoshootettoit :hd th otothf  wa thy th the thntishhltyrthes to;le thag \n",
      "----\n",
      "iter 200, loss: 99.263991\n",
      "----\n",
      " yucayuvou tyuyruy aauer loues yuue prv\n",
      "cyvudu thoye\n",
      "y ryeesu youyeosaon bu y ret ueudyuOeurragyryarurr uro lee chysc oeu yonoumy wyuyouyNyeunuryyueo  th sout vurna ue\n",
      "geeruy seou sey eNlyrunfrT\n",
      "no sou \n",
      "----\n",
      "iter 300, loss: 96.521589\n",
      "----\n",
      " fLof uof. gos ihllhoulkd biue h? svid. ithai athe y bauef bity Tsny dh if ffaf hos fnat hh mshe i whhsos ny ghltarush? fig? seco bed wour eiende thoa hou satr  on ou sonl uos shok iou hog oon bes mhir \n",
      "----\n",
      "iter 400, loss: 93.919796\n",
      "----\n",
      " Aed it thoter Whnqbtd pf wtes vfe ays,\n",
      "se e?ue tuvs Non ansed sule hn hues\n",
      ",nns\n",
      " un ne Aids wim Bwob  Med cee;\n",
      "Theed Ay en .\n",
      "Bnorcr'ivr outee\n",
      "the theumsee thie bnfod sot eheee ss. aut bup  wa nomed an \n",
      "----\n",
      "iter 500, loss: 91.322040\n",
      "----\n",
      " h gir, nso tpasl fhout poe toch bhatochatimh yup: butan fonsdons btho coNtlon, otb ou,\n",
      "low iushics of warth bok toon fourd iry auge ur lhis mnld Iouror hols,\n",
      "Woax pato ley, woan's,\n",
      "hon so thr shan,\n",
      "Fo \n",
      "----\n",
      "iter 600, loss: 88.677460\n",
      "----\n",
      " eudiFisdrfsote,\n",
      "mireildss fecaroeiddrulhliwtinkhk\n",
      "ligs s'ees Iker sey si eimk: pocild insddile'dplsthessgs in athedgsars ert,\n",
      "kekd's,\n",
      "Fog Dm :\n",
      "Fimse,\n",
      "wis lh.\n",
      "Wise dirss ilie siAl thir chysersgs men ch \n",
      "----\n",
      "iter 700, loss: 86.285126\n",
      "----\n",
      "  theld \n",
      "Tros sle buch,\n",
      "And hct,\n",
      "And ot apid fald det eich'd owl sikoth nacoy t thg mhe fed mce mor tt aftgath funson siceswuicttfrid Rfse loudadd detard chem helt feiins womeghash harer wlAvired ifr a \n",
      "----\n",
      "iter 800, loss: 83.954713\n",
      "----\n",
      " hasmecleadeapt,,s dimens,\n",
      "mat,,\n",
      "Ye bire th rowet meugheis,sathanm,\n",
      "Sn anowrin beor ord rengle srovelm Ion Ieemecg: sire:\n",
      "Eig, aI, imore,\n",
      "Ae nI thtthald hon, thte,\n",
      "koth\n",
      "Soretar oUd lar biot, oucofmon,  \n",
      "----\n",
      "iter 900, loss: 81.782349\n",
      "----\n",
      " h!it frly tith seesy canfagty beitt;\n",
      "Fou, tovy\n",
      "A dot nar thou yyirenceytwimeir urids.\n",
      "Ty futeydn wy bre thysunwsty yenags wa'a,\n",
      "Bu whon thov sortermy onrr thy breawiftmrvefin atle eiss\n",
      "widd mywhy! mey \n",
      "----\n",
      "iter 1000, loss: 79.581856\n",
      "----\n",
      " ,\n",
      "U,\n",
      "pon my,\n",
      "Enees leme,\n",
      "dome,\n",
      "Tree ponj Iy tevheg,\n",
      "by be woise, bef,\n",
      "les rasongind bug;\n",
      "Whet\n",
      "Ne ferce,\n",
      "Bon Iepof owe,\n",
      "Ans grl\n",
      "\n",
      "Theees'bnt so thoud mn;\n",
      "When w\n",
      "d mey whas owpe gefecs fy ne tee eure tiv \n",
      "----\n",
      "iter 1100, loss: 77.550439\n",
      "----\n",
      " \n",
      "Tiari lhont,\n",
      "Fir d on hn bs wiins mart pach\n",
      "Arts\n",
      "Tige rish nar undr o';\n",
      "Mout thim thireund y hher ,oring pise ars no rarincd whimart lete\n",
      "n dess ros sr th biar,\n",
      "Whim toe ot,\n",
      "Wuld my prar\n",
      "fonle y to t \n",
      "----\n",
      "iter 1200, loss: 75.694163\n",
      "----\n",
      "  ime wis thou't fome aft aroighe dille? thire tore?\n",
      "Hid.\n",
      "Tf iDe'e bl'se;\n",
      "So glin be\n",
      "Thet the tifs fa, wite yobgaret lihe-ttDnpis. sigee sude tMeld, sume thes.\n",
      "Dos itpanst.\n",
      "Tuae onse beamre?\n",
      "Audpasse,  \n",
      "----\n",
      "iter 1300, loss: 74.044245\n",
      "----\n",
      " sedt? twou hr my our?\n",
      "Sodt tht,\n",
      "Soy cimg mosh thouase uy, lomasu,w nosS cos beapSot plee rhou ceage thouss sog?\n",
      "Thy pragess tacroukrstnlues'f, Thek toind wou thom lon mut iur ey the thing tiou thise s \n",
      "----\n",
      "iter 1400, loss: 72.686586\n",
      "----\n",
      "  weld rlas I that leay ley thom nu are thitte-lire sdout cre bov thas cor ds shist:\n",
      "An thou dombr tote; dlrm thour;\n",
      "br earawe th  he thof mou hory guy bord, rhin tort ank! tird, se d dftttond chredn f \n",
      "----\n",
      "iter 1500, loss: 71.150357\n",
      "----\n",
      " ough thrt belyswslinf\n",
      "Thesipritt scie.\n",
      "Thou shorses tingisuat mouuper fifs wky wir wudcichld geeges l? tout twalh in houtuss dndtssthpthee spat slb hes nw snollowsinmend wilemsst dlotese, witem wifute \n",
      "----\n",
      "iter 1600, loss: 69.977936\n",
      "----\n",
      " , wirat tor brd lo wit.\n",
      "Asheas hos loou thebwaf trelp the'd tharl magits atc;\n",
      "Mrsrear qwt dofa heat apth nouls in d'd uwiked wat yourid-' top deau stivy ner in nitheMur my, wit tet  riws thatAp'd thav \n",
      "----\n",
      "iter 1700, loss: 68.812063\n",
      "----\n",
      " or wot thar tougisp wor suftiveld corllle that, thiw\n",
      "dowrwmy ove thaud nouth srot thso soupro sst bethe\n",
      "Oulens,\n",
      "I, thy dot sot ow it eipow wost.\n",
      "Awtret erereth witung ict;\n",
      "Wit timh,\n",
      "Made\n",
      "Thirearath\n",
      "My \n",
      "----\n",
      "iter 1800, loss: 67.572429\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ef701bf9ebf9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m   \u001b[1;31m# forward seq_length characters through the net and fetch gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m   \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWxh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlossFun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m   \u001b[0msmooth_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.999\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'iter %d, loss: %f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msmooth_loss\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# print progress\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-ef701bf9ebf9>\u001b[0m in \u001b[0;36mlossFun\u001b[1;34m(inputs, targets, hprev)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0mdhraw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdh\u001b[0m \u001b[1;31m# backprop through tanh nonlinearity\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mdbh\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdhraw\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[0mdWxh\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdhraw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[0mdWhh\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdhraw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mdhnext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWhh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdhraw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('input.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print 'data has %d characters, %d unique.' % (data_size, vocab_size)\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in xrange(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(xrange(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\"\n",
    "  sample a sequence of integers from the model\n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  #probability of current char is 1.  all others 0\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in xrange(n):\n",
    "    #from current letter, predict probs of next letter\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    #take a random draw with predicted prob wts\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0:\n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 100 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 200)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print '----\\n %s \\n----' % (txt, )\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 100 == 0: print 'iter %d, loss: %f' % (n, smooth_loss) # print progress\n",
    "\n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
