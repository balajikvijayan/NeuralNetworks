{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Lecture 4.3: Classifying CIFAR with CNN\n",
    "This lecture will focus on adapting the CNN we've been using for MNIST to processing color images.  The class will use the Cifar-10 images as an example\n",
    "\n",
    "##Learning Objectives\n",
    "1.  Become familiar and comfortable with reshaping tensors\n",
    "2.  Use Theano to build an image classifier\n",
    "\n",
    "##Order of topics covered\n",
    "1.  Review storage of vectors and matrices (and tensors)\n",
    "2.  Work some numpy examples \n",
    "3.  Modify the CNN code to train on Cifar-10 data\n",
    "4.  Start the training process on Cifar-10\n",
    "\n",
    "##Pre-reading\n",
    "https://en.wikipedia.org/wiki/Row-major_order\n",
    "\n",
    "\n",
    "##How vectors and matrices are stored\n",
    "Why should you learn about how vectors and matrices are stored?  You've seen in the CNN code for classifying MNIST digits that the theano reshape function was used several times.  Each example image was stored as a one-dimensional array in the data file.  That was fine for a fully connected input layer, but for a convolutional layer it had to be converted to a 28x28 image-shaped matrix.  Then the final 128x3x3 tensor had to be converted back to a vector for input to the first fully connected layer.  In order to get the pixels where you want them you need to understand how matrices and tensors get reshaped.  The easiest way to understand reshaping is to understand how matrices and tensors get reshaped into vectors, which is how they get stored.  Visualizing the reshape process as converting from the first shape to a vector and then from a vector to the second shape will help you keep things straight.  \n",
    "\n",
    "\n",
    "It's easy to understand how a vector might be stored.  The first element is written into a memory location.  The second element is written next to the first and each successive element is written next to the last one.  A matrix is written into memory the same way.  After the first element is written, each successive element is written next to the last.  It is natural to start with the 1,1 element, but what element comes next?  It could either be 1,2 or 2,1.  Storing the 1,2 element next would be serializing the matrix row-by-row.  Taking 2,1 would be serializing column.  Another way to look at it is that the matrix has two indices (j, k).  Does the first index count faster or does the second.  Taking 1,2 as the second element (or storing row-by-row) would be counting through the second index fastest.  In fact different languages take different approaches.  See https://en.wikipedia.org/wiki/Row-major_order.  As you can see in that wikipedia article Fortran uses column-major storage where the first index runs fastest whereas C uses row-major storage where the second index runs faster.  How can you determine the order for Python?  \n",
    "\n",
    "##In-class Coding Exercises\n",
    "1.  Form a one-dimensional numpy array that is filled with counting numbers.  Reshape the resulting one-dimensional array into a matrix to observe whether Python is row-major or column-major.  \n",
    "\n",
    "2.  Confirm that reshaping operations can always be understood by reference to the way the original and the reshaped matrices (tensors) would be stored as vectors.  Do this by starting with a vector that you can reshape into more than one matrix shape.  Reshape it into one of these shapes.  Reshape the result into the second and confirm that you'd have gotten the same result by reshaping the starting vector into the second shape.  \n",
    "\n",
    "3.  Go through the same exercise as 2 where one of the two shapes is a 3-dimensional tensor. \n",
    "4.  Adapt the convolution neural net that you saw last lecture to process the Cifar-10 image collection.  The code is repeated below.  The function used to read in the MNIST data has been changed to read Cifar-10 data.  You'll need to make the other required changes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "import numpy as np\n",
    "from cifarHandler import cifar\n",
    "from theano.tensor.nnet.conv import conv2d\n",
    "from theano.tensor.signal.downsample import max_pool_2d\n",
    "\n",
    "srng = RandomStreams()\n",
    "\n",
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def init_weights(shape):\n",
    "    return theano.shared(floatX(np.random.randn(*shape) * 0.01))\n",
    "\n",
    "def rectify(X):\n",
    "    return T.maximum(X, 0.)\n",
    "    #return T.maximum(X, 0.01*X)  #leaky rectifier\n",
    "\n",
    "def softmax(X):\n",
    "    e_x = T.exp(X - X.max(axis=1).dimshuffle(0, 'x'))\n",
    "    return e_x / e_x.sum(axis=1).dimshuffle(0, 'x')\n",
    "\n",
    "def dropout(X, p=0.0):\n",
    "    if p > 0:\n",
    "        retain_prob = 1 - p\n",
    "        X *= srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n",
    "        X /= retain_prob\n",
    "    return X\n",
    "\n",
    "def RMSprop(cost, params, lr=0.001, rho=0.9, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        gradient_scaling = T.sqrt(acc_new + epsilon)\n",
    "        g = g / gradient_scaling\n",
    "        updates.append((acc, acc_new))\n",
    "        updates.append((p, p - lr * g))\n",
    "    return updates\n",
    "\n",
    "def model(X, w, w2, w3, w4, p_drop_conv, p_drop_hidden):\n",
    "    l1a = rectify(conv2d(X, w, border_mode='full'))\n",
    "    l1 = max_pool_2d(l1a, (2, 2))\n",
    "    l1 = dropout(l1, p_drop_conv)\n",
    "\n",
    "    l2a = rectify(conv2d(l1, w2))\n",
    "    l2 = max_pool_2d(l2a, (2, 2))\n",
    "    l2 = dropout(l2, p_drop_conv)\n",
    "\n",
    "    l3a = rectify(conv2d(l2, w3))\n",
    "    l3b = max_pool_2d(l3a, (2, 2))\n",
    "    l3 = T.flatten(l3b, outdim=2)\n",
    "    l3 = dropout(l3, p_drop_conv)\n",
    "\n",
    "    l4 = rectify(T.dot(l3, w4))\n",
    "    l4 = dropout(l4, p_drop_hidden)\n",
    "\n",
    "    pyx = softmax(T.dot(l4, w_o))\n",
    "    return l1, l2, l3, l4, pyx\n",
    "\n",
    "\n",
    "xTrain, yTrain, xTest, yTest = cifar(nData=2, Normalize=True)\n",
    "xTrain = xTrain.reshape(-1, 1, 28, 28)\n",
    "xTest = xTest.reshape(-1, 1, 28, 28)\n",
    "\n",
    "X = T.ftensor4()\n",
    "Y = T.fmatrix()\n",
    "\n",
    "w = init_weights((32, 1, 3, 3))\n",
    "w2 = init_weights((64, 32, 3, 3))\n",
    "w3 = init_weights((128, 64, 3, 3))\n",
    "w4 = init_weights((128 * 3 * 3, 625))\n",
    "w_o = init_weights((625, 10))\n",
    "\n",
    "noise_l1, noise_l2, noise_l3, noise_l4, noise_py_x = model(X, w, w2, w3, w4, 0.2, 0.5)\n",
    "l1, l2, l3, l4, py_x = model(X, w, w2, w3, w4, 0., 0.)\n",
    "y_x = T.argmax(py_x, axis=1)\n",
    "\n",
    "\n",
    "cost = T.mean(T.nnet.categorical_crossentropy(noise_py_x, Y))\n",
    "params = [w, w2, w3, w4, w_o]\n",
    "updates = RMSprop(cost, params, lr=0.001)\n",
    "\n",
    "train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)\n",
    "predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True)\n",
    "\n",
    "for i in range(100):\n",
    "    for start, end in zip(range(0, len(xTrain), 128), range(128, len(xTrain), 128)):\n",
    "        cost = train(xTrain[start:end], yTrain[start:end])\n",
    "        #a, b, c, d, e = model(floatX(trX[start:end]), w, w2, w3, w4, 0., 0.)\n",
    "        #print a.eval().shape, b.eval().shape, c.eval().shape, d.eval().shape\n",
    "    print np.mean(np.argmax(yTest, axis=1) == predict(xTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
